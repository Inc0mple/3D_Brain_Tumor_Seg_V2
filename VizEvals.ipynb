{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "\n",
    "from IPython.display import Image as show_gif\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Meter import dice_coef_metric_per_classes, jaccard_coef_metric_per_classes\n",
    "\n",
    "# CAN POTENTIALLY TRY DIFFERENT AUGMENTATION TECHNIQUES HERE\n",
    "from utils.BratsDataset import BratsDataset\n",
    "\n",
    "from utils.Meter import BCEDiceLoss\n",
    "\n",
    "from models.UNet3d import UNet3d\n",
    "from models.UNet3dSingleConv import UNet3dSingleConv\n",
    "from models.UNet3dDropout import UNet3dDropout\n",
    "from models.SwinUNETR import SwinUNETR\n",
    "from models.UNet3d_your_modifications import UNet3d_your_modifications\n",
    "from models.UNet3d_SELU import UNet3d_SELU\n",
    "from models.UNet3d_atten import UNet3d_atten\n",
    "from models.ONet3d import ONet3d\n",
    "from models.ONet3d_v2 import ONet3d_v2\n",
    "from models.ONet3d_v3 import ONet3d_v3\n",
    "from models.ONet3d_v3_DoubleConv import ONet3d_v3_DoubleConv\n",
    "from models.UNet3d_GELU import UNet3d_GELU\n",
    "from models.ONet3d_v3_GELU import ONet3d_v3_GELU\n",
    "from models.SphereNet3d import SphereNet3d\n",
    "from models.SphereNet3d import SphereNet3d\n",
    "# from models.SphereNet3d_GELU import SphereNet3d_GELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    path_to_csv: str,\n",
    "    # phase: str,\n",
    "    val_fold: int = 0,  # Choose which fold to be the validation fold\n",
    "    test_fold: int = 1,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4,\n",
    "    do_resizing: bool = True,\n",
    "):\n",
    "    assert (val_fold != test_fold)\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    '''Returns: dataloader for the model training'''\n",
    "    # Data in folds other than 0 are used for training\n",
    "    train_df = df.loc[~df['fold'].isin(\n",
    "        [val_fold, test_fold])].reset_index(drop=True)\n",
    "    # Data in fold 0 is used for validation\n",
    "    val_df = df.loc[df['fold'] == val_fold].reset_index(drop=True)\n",
    "    test_df = df.loc[df['fold'] == test_fold].reset_index(drop=True)\n",
    "\n",
    "    # dataset = dataset(df, phase)\n",
    "    train_dataset = dataset(train_df, \"train\", do_resizing=do_resizing)\n",
    "    val_dataset = dataset(val_df, \"val\", do_resizing=do_resizing)\n",
    "    test_dataset = dataset(test_df, \"test\", do_resizing=do_resizing)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDict = {\n",
    "    \"3DOnet_DoubleConv_Kernel1\": ONet3d_v3_DoubleConv(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1\": ONet3d_v3(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_32_Channels\": ONet3d_v3(in_channels=4, n_classes=3, n_channels=32).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_GELU\": ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_GELU_AdamW\": ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel3\": ONet3d_v2(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel5\": ONet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet\": UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_32_Channels\": UNet3d(in_channels=4, n_classes=3, n_channels=32).to('cuda'),\n",
    "    \"3DUnet_Atten\": UNet3d_atten(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_Dropout\": UNet3dDropout(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_GELU\": UNet3d_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_SELU\": UNet3d_SELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_SingleConv\": UNet3dSingleConv(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"SphereNet3D\": SphereNet3d(in_channels=4, n_classes=3, n_channels=16).to('cuda'),\n",
    "    \"SwinUNETR\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "    \"SwinUNETR_AdamW\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "    \"SwinUNETR_DoubleLayerDepth\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(2, 2, 2, 2), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_per_classes(model,\n",
    "                               dataloader,\n",
    "                               classes):\n",
    "    \"\"\"\n",
    "    Compute Dice and Jaccard coefficients for each class.\n",
    "    Params:\n",
    "        model: neural net for make predictions.\n",
    "        dataloader: dataset object to load data from.\n",
    "        classes: list with classes.\n",
    "        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dice_scores_per_classes = {key: list() for key in classes}\n",
    "    iou_scores_per_classes = {key: list() for key in classes}\n",
    "    ids = {\"Ids\":list()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            imgs, targets = data['image'], data['mask']\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            dice_scores = dice_coef_metric_per_classes(logits, targets)\n",
    "            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n",
    "            ids[\"Ids\"].extend(data[\"Id\"])\n",
    "\n",
    "            for key in dice_scores.keys():\n",
    "                dice_scores_per_classes[key].extend(dice_scores[key])\n",
    "\n",
    "            for key in iou_scores.keys():\n",
    "                iou_scores_per_classes[key].extend(iou_scores[key])\n",
    "    return dice_scores_per_classes, iou_scores_per_classes, ids\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "def evaluate_all(chkpt_type=\"best\", results_dict = {}):\n",
    "  chkpt_prefix = \"your_best_model\" if chkpt_type == \"best\" else \"your_last_epoch_model\"\n",
    "  for dirname in os.listdir(\"Logs\"):\n",
    "    start = datetime.now()\n",
    "    model_name = dirname\n",
    "    model = modelDict[model_name]\n",
    "    results_dict[model_name] = {}\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    \n",
    "    for filename in os.listdir(f\"Logs/{dirname}\"):\n",
    "      if chkpt_prefix in filename:\n",
    "        checkpoint_path = f\"Logs/{dirname}/{filename}\"\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "      model.load_state_dict(torch.load(\n",
    "          checkpoint_path, map_location='cpu'))\n",
    "      model.eval()\n",
    "      print(f\"{model_name} loaded with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading {model_name} with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "      print(e)\n",
    "      \n",
    "    _, _, test_dataloader = get_dataloaders(\n",
    "        dataset=BratsDataset, path_to_csv=\"./fold_data.csv\", val_fold=0, test_fold=1, batch_size=1, do_resizing=True)\n",
    "    \n",
    "    dice_scores_per_classes, iou_scores_per_classes, ids = compute_scores_per_classes(\n",
    "        model, test_dataloader, ['WT', 'TC', 'ET']\n",
    "    )\n",
    "    ids_df = pd.DataFrame(ids)\n",
    "    ids_df.columns = ['Ids']\n",
    "    \n",
    "    dice_df = pd.DataFrame(dice_scores_per_classes)\n",
    "\n",
    "\n",
    "    dice_df.columns = ['WT dice', 'TC dice', 'ET dice']\n",
    "\n",
    "    iou_df = pd.DataFrame(iou_scores_per_classes)\n",
    "    iou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard']\n",
    "    val_metics_df = pd.concat([ids_df, dice_df, iou_df], axis=1, sort=True)\n",
    "    val_metics_df = val_metics_df.loc[:, ['Ids',\n",
    "                                          'WT dice', 'WT jaccard',\n",
    "                                          'TC dice', 'TC jaccard',\n",
    "                                          'ET dice', 'ET jaccard']]\n",
    "    mean_metrics_df = val_metics_df.mean()\n",
    "    val_metics_df.sort_values(by=[\"Ids\"], inplace=True)\n",
    "    val_metics_df = val_metics_df.reset_index(drop=True)\n",
    "    results_dict[model_name][\"val_metics_df\"] = val_metics_df\n",
    "    results_dict[model_name][\"WT dice\"] = mean_metrics_df[\"WT dice\"]\n",
    "    results_dict[model_name][\"WT jaccard\"] = mean_metrics_df[\"WT jaccard\"]\n",
    "    results_dict[model_name][\"TC dice\"] = mean_metrics_df[\"TC dice\"]\n",
    "    results_dict[model_name][\"TC jaccard\"] = mean_metrics_df[\"TC jaccard\"]\n",
    "    results_dict[model_name][\"ET dice\"] = mean_metrics_df[\"ET dice\"]\n",
    "    results_dict[model_name][\"ET jaccard\"] = mean_metrics_df[\"ET jaccard\"]\n",
    "    inference_time = datetime.now() - start\n",
    "    results_dict[model_name][\"Inference time\"] = inference_time\n",
    "    \n",
    "    del model\n",
    "  return results_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3DOnet_SingleConv_Kernel1_32_Channels loaded with chkpt: Logs/3DOnet_SingleConv_Kernel1_32_Channels/your_last_epoch_model_20230314-222549.pth. parameters: 5597571\n",
      "SphereNet3D loaded with chkpt: Logs/SphereNet3D/your_last_epoch_model_20230319-011358.pth. parameters: 6134019\n",
      "3DUnet_SingleConv loaded with chkpt: Logs/3DUnet_SingleConv/your_last_epoch_model_20230322-164951.pth. parameters: 3006507\n",
      "3DOnet_SingleConv_Kernel1_GELU_AdamW loaded with chkpt: Logs/3DOnet_SingleConv_Kernel1_GELU_AdamW/your_last_epoch_model_20230317-195004.pth. parameters: 3150435\n",
      "SwinUNETR_AdamW loaded with chkpt: Logs/SwinUNETR_AdamW/your_last_epoch_model_20230311-072307.pth. parameters: 14981601\n",
      "3DUnet_Atten loaded with chkpt: Logs/3DUnet_Atten/your_last_epoch_model_20230311-222850.pth. parameters: 6094231\n",
      "3DOnet_SingleConv_Kernel1 loaded with chkpt: Logs/3DOnet_SingleConv_Kernel1/your_last_epoch_model_20230313-220534.pth. parameters: 3150435\n",
      "SwinUNETR loaded with chkpt: Logs/SwinUNETR/your_last_epoch_model_20230311-001805.pth. parameters: 14981601\n",
      "3DOnet_DoubleConv_Kernel1 loaded with chkpt: Logs/3DOnet_DoubleConv_Kernel1/your_last_epoch_model_20230314-063755.pth. parameters: 5896515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28125/1319466525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dict_last_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchkpt_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28125/3217931552.py\u001b[0m in \u001b[0;36mevaluate_all\u001b[0;34m(chkpt_type, results_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         dataset=BratsDataset, path_to_csv=\"./fold_data.csv\", val_fold=0, test_fold=1, batch_size=1, do_resizing=True)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     dice_scores_per_classes, iou_scores_per_classes, ids = compute_scores_per_classes(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'WT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipykernel_28125/431513174.py\u001b[0m in \u001b[0;36mcompute_scores_per_classes\u001b[0;34m(model, dataloader, classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_dict_last_epoch = evaluate_all(chkpt_type=\"last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict_best_epoch = evaluate_all(chkpt_type=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/results_last_epoch.txt\", \"w\") as outfile:\n",
    "    outfile.write(str(eval_dict_last_epoch))\n",
    "    \n",
    "with open(\"results/results_best_epoch.txt\", \"w\") as outfile:\n",
    "    outfile.write(str(eval_dict_best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict_last_epoch_trunc = eval_dict_last_epoch.copy()\n",
    "for k in eval_dict_last_epoch_trunc:\n",
    "  del eval_dict_last_epoch_trunc[k][\"val_metics_df\"]\n",
    "eval_dict_last_epoch_trunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/eval_last_epoch.txt\", \"w\") as outfile:\n",
    "    outfile.write(str(eval_dict_last_epoch))\n",
    "\n",
    "with open(\"results/eval_best_epoch.txt\", \"w\") as outfile:\n",
    "    outfile.write(str(eval_dict_best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_losses(filename):\n",
    "\n",
    "#   # open the text file\n",
    "#   with open(filename, 'r') as f:\n",
    "#       contents = f.read()\n",
    "\n",
    "#   # extract the train losses and val losses using regular expressions\n",
    "#   train_losses_str = re.search(\n",
    "#       r\"losses:{'train': (.+?), 'val'\", contents).group(1)\n",
    "#   val_losses_str = re.search(r\"'val': \\[(.+?)\\}\", contents).group(1)\n",
    "\n",
    "#   train_losses_str = train_losses_str.replace('[', '').replace(']', '')\n",
    "#   val_losses_str = val_losses_str.replace('[', '').replace(']', '')\n",
    "\n",
    "#   # convert the strings to lists of floating-point numbers\n",
    "#   train_losses = [float(x.strip()) for x in train_losses_str.split(', ')]\n",
    "#   val_losses = [float(x.strip()) for x in val_losses_str.split(', ')]\n",
    "\n",
    "#   print(f\"{len(train_losses)} train and {len(val_losses)} val losses found in {filename}\")\n",
    "\n",
    "#   # print the lists\n",
    "#   return train_losses, val_losses\n",
    "\n",
    "\n",
    "# def get_param_count(filename):\n",
    "\n",
    "#   # open the text file\n",
    "#   with open(filename, 'r') as f:\n",
    "#       contents = f.read()\n",
    "\n",
    "#   # extract the param count using regular expressions\n",
    "#   param_str = re.search(\n",
    "#       r\"(?<=parameter_count:).*\", contents).group(0)\n",
    "#   param_count = int(param_str)\n",
    "#   print(f\"Parameter count = {param_count} in {filename}\")\n",
    "\n",
    "#   return param_count\n",
    "\n",
    "\n",
    "# def get_dice_scores(filename):\n",
    "\n",
    "#   # open the text file\n",
    "#   with open(filename, 'r') as f:\n",
    "#       lines = f.readlines()\n",
    "\n",
    "#   for line in lines:\n",
    "#     if \"dice_scores\" in line:\n",
    "\n",
    "#       # extract the dice scores and iou scores using regular expressions\n",
    "#       train_dice_str = re.search(\n",
    "#           r\"'train': (.+?), 'val'\", line).group(1)\n",
    "#       val_dice_str = re.search(r\"'val': \\[(.+?)\\}\", line).group(1)\n",
    "\n",
    "#       train_dice_str = train_dice_str.replace('[', '').replace(']', '')\n",
    "#       val_dice_str = val_dice_str.replace('[', '').replace(']', '')\n",
    "\n",
    "#       # convert the strings to lists of floating-point numbers\n",
    "#       train_dice = [float(x.strip()) for x in train_dice_str.split(', ')]\n",
    "#       val_dice = [float(x.strip()) for x in val_dice_str.split(', ')]\n",
    "#       break\n",
    "#   print(f\"{len(train_dice)} train and {len(val_dice)} val dice score found in {filename}\")\n",
    "#   return train_dice, val_dice\n",
    "\n",
    "\n",
    "# def get_jaccard_scores(filename):\n",
    "\n",
    "#   # open the text file\n",
    "#   with open(filename, 'r') as f:\n",
    "#       lines = f.readlines()\n",
    "\n",
    "#   for line in lines:\n",
    "#     if \"jaccard_scores\" in line:\n",
    "\n",
    "#       # extract the dice scores and iou scores using regular expressions\n",
    "#       train_jaccard_str = re.search(\n",
    "#           r\"'train': (.+?), 'val'\", line).group(1)\n",
    "#       val_jaccard_str = re.search(r\"'val': \\[(.+?)\\}\", line).group(1)\n",
    "\n",
    "#       train_jaccard_str = train_jaccard_str.replace('[', '').replace(']', '')\n",
    "#       val_jaccard_str = val_jaccard_str.replace('[', '').replace(']', '')\n",
    "\n",
    "#       # convert the strings to lists of floating-point numbers\n",
    "#       train_jaccard = [float(x.strip()) for x in train_jaccard_str.split(', ')]\n",
    "#       val_jaccard = [float(x.strip()) for x in val_jaccard_str.split(', ')]\n",
    "#       break\n",
    "#   print(f\"{len(train_jaccard)} train and {len(val_jaccard)} val jaccard score found in {filename}\")\n",
    "#   return train_jaccard, val_jaccard\n",
    "\n",
    "\n",
    "# def get_train_run_time(filename):\n",
    "\n",
    "#   # open the text file\n",
    "#   with open(filename, 'r') as f:\n",
    "#       contents = f.read()\n",
    "\n",
    "#   # extract the param count using regular expressions\n",
    "#   time_str = re.search(\n",
    "#       r\"(?<=last_completed_run_time:).*(?=\\n)\", contents).group(0)\n",
    "#   time_obj = datetime.strptime(time_str, \"%H:%M:%S.%f\").time()\n",
    "#   print(f\"Trainer runtime = {time_obj} in {filename}\")\n",
    "#   return time_obj\n",
    "\n",
    "from utils.viz_eval_utils import get_losses, get_param_count, get_dice_scores, get_jaccard_scores, get_train_run_time\n",
    "\n",
    "\n",
    "FILENAME = \"Logs/3DOnet_DoubleConv_Kernel1/trainer_properties.txt\"\n",
    "\n",
    "get_param_count(FILENAME)\n",
    "get_train_run_time(FILENAME)\n",
    "get_losses(FILENAME)\n",
    "get_dice_scores(FILENAME)\n",
    "get_jaccard_scores(FILENAME)\n",
    "print(\"Test done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dict = {}\n",
    "\n",
    "for dirname in os.listdir(\"Logs\"):\n",
    "  model_name = dirname\n",
    "  train_results_dict[model_name] = {}\n",
    "\n",
    "  filename = f\"Logs/{dirname}/trainer_properties.txt\"\n",
    "  parameter_count = get_param_count(filename)\n",
    "  trainer_runtime = get_train_run_time(filename)\n",
    "  train_losses, val_losses = get_losses(filename)\n",
    "  train_dices, val_dices = get_dice_scores(filename)\n",
    "  train_jaccards, val_jacards = get_jaccard_scores(filename)\n",
    "  train_results_dict[model_name][\"parameter_count\"] = parameter_count\n",
    "  train_results_dict[model_name][\"trainer_runtime\"] = trainer_runtime\n",
    "  train_results_dict[model_name][\"train_losses\"] = train_losses\n",
    "  train_results_dict[model_name][\"val_losses\"] = val_losses\n",
    "  train_results_dict[model_name][\"train_dices\"] = train_dices\n",
    "  train_results_dict[model_name][\"val_dices\"] = val_dices\n",
    "  train_results_dict[model_name][\"train_jaccards\"] = train_jaccards\n",
    "  train_results_dict[model_name][\"val_jacards\"] = val_jacards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain color palette with shades corresponding to parameter counts\n",
    "\n",
    "Unet_colors = [\"#D8E8FA\", \"#CCE0F5\", \"#B2D2F7\",\n",
    "               \"#91C2FA\", \"#69ADF9\", \"#3E97FA\", \"#117FFB\"]  # blue\n",
    "# Onet_colors = [\"#F9E0ED\", \"#F9C7E1\", \"#F9A5D1\", \"#FB7CBE\", \"#FC57AC\", \"#FA2693\", \"#DE0977\"] # pink\n",
    "Onet_colors = [\"#F4E6F7\", \"#F0CCF9\", \"#EBABF9\",\n",
    "               \"#E483FA\", \"#DC4AFC\", \"#CF0CFA\", \"#9703B8\"]  # purple\n",
    "Swin_colors = [\"#D3F3C4\", \"#A3F47D\", \"#44C10A\"]  # green\n",
    "spherenet_colors = [\"#FC9A03\"]  # orange\n",
    "Unet_dict = {}\n",
    "Onet_dict = {}\n",
    "Swin_dict = {}\n",
    "spherenet_dict = {}\n",
    "for model_name in train_results_dict:\n",
    "    if \"Unet\" in model_name:\n",
    "        Unet_dict[model_name] = train_results_dict[model_name][\"parameter_count\"]\n",
    "    elif \"Onet\" in model_name:\n",
    "        Onet_dict[model_name] = train_results_dict[model_name][\"parameter_count\"]\n",
    "    elif \"Swin\" in model_name:\n",
    "        Swin_dict[model_name] = train_results_dict[model_name][\"parameter_count\"]\n",
    "    else:\n",
    "        spherenet_dict[model_name] = train_results_dict[model_name][\"parameter_count\"]\n",
    "\n",
    "sorted_Unet_models = sorted(Unet_dict.items(), key=lambda x: x[1])\n",
    "sorted_Onet_models = sorted(Onet_dict.items(), key=lambda x: x[1])\n",
    "sorted_Swin_models = sorted(Swin_dict.items(), key=lambda x: x[1])\n",
    "sorted_spherenet_models = sorted(spherenet_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "cols = {}\n",
    "for i in range(len(sorted_Unet_models)):\n",
    "    cols[sorted_Unet_models[i][0]] = Unet_colors[i]\n",
    "for i in range(len(sorted_Onet_models)):\n",
    "    cols[sorted_Onet_models[i][0]] = Onet_colors[i]\n",
    "for i in range(len(sorted_Swin_models)):\n",
    "    cols[sorted_Swin_models[i][0]] = Swin_colors[i]\n",
    "for i in range(len(sorted_spherenet_models)):\n",
    "    cols[sorted_spherenet_models[i][0]] = spherenet_colors[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_param_count(results_dict, paletteCols):\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in results_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(results_dict[model_name][\"parameter_count\"])\n",
    "\n",
    "#   # sort based on ascending order of parameter count\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Parameter Count\")\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"Parameter Count\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_train_losses(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"train_losses\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   train_loss_data = {'Epochs': epoch_num_xlist}\n",
    "#   train_loss_df = pd.DataFrame(train_loss_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     train_loss_df[model_name] = results_dict[model_name][\"train_losses\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Train Losses\")\n",
    "#   sns.lineplot(data=train_loss_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Train Losses\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_val_losses(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"val_losses\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   val_loss_data = {'Epochs': epoch_num_xlist}\n",
    "#   val_loss_df = pd.DataFrame(val_loss_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     val_loss_df[model_name] = results_dict[model_name][\"val_losses\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Validation Losses\")\n",
    "#   sns.lineplot(data=val_loss_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Validation Losses\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_train_dices(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"train_dices\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   train_dices_data = {'Epochs': epoch_num_xlist}\n",
    "#   train_dices_df = pd.DataFrame(train_dices_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     train_dices_df[model_name] = results_dict[model_name][\"train_dices\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Train Dice Scores\")\n",
    "#   sns.lineplot(data=train_dices_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Train Dice Scores\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_val_dices(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"val_dices\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   val_dices_data = {'Epochs': epoch_num_xlist}\n",
    "#   val_dices_df = pd.DataFrame(val_dices_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     val_dices_df[model_name] = results_dict[model_name][\"val_dices\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Validation Dice Scores\")\n",
    "#   sns.lineplot(data=val_dices_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Validation Dice Scores\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_train_jaccards(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"train_jaccards\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   train_jaccards_data = {'Epochs': epoch_num_xlist}\n",
    "#   train_jaccards_df = pd.DataFrame(train_jaccards_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     train_jaccards_df[model_name] = results_dict[model_name][\"train_jaccards\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Train Jaccard Scores\")\n",
    "#   sns.lineplot(data=train_jaccards_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Train Jaccard Scores\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_val_jacards(results_dict, paletteCols):\n",
    "\n",
    "#   epoch_num = len(results_dict[\"3DOnet_DoubleConv_Kernel1\"][\"val_jacards\"])\n",
    "#   epoch_num_xlist = list(range(1, epoch_num + 1))\n",
    "#   val_jacards_data = {'Epochs': epoch_num_xlist}\n",
    "#   val_jacards_df = pd.DataFrame(val_jacards_data)\n",
    "\n",
    "#   for model_name in results_dict:\n",
    "#     val_jacards_df[model_name] = results_dict[model_name][\"val_jacards\"]\n",
    "\n",
    "#   models_ls = []\n",
    "#   for model_name in results_dict:\n",
    "#     models_ls.append(model_name)\n",
    "\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Validation Jaccard Scores\")\n",
    "#   sns.lineplot(data=val_jacards_df[models_ls], linewidth=3, palette=paletteCols)\n",
    "#   ax.set_xlabel(\"Epochs\")\n",
    "#   ax.set_ylabel(\"Validation Jaccard Scores\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   plt.show()\n",
    "\n",
    "# def plot_trainer_runtime(results_dict, paletteCols):\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in results_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(results_dict[model_name][\"trainer_runtime\"])\n",
    "\n",
    "#   # sort based on ascending order of parameter count\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   y = [round(i.hour+(i.minute/60), 2) for i in y]\n",
    "#   # today = datetime.datetime.now()\n",
    "#   # y = [datetime.datetime.combine(today, t) for t in y]\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"Model Trainer Runtime (hours)\")\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "#   # plt.bar(x, y)\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"Trainer runtime in hours\")\n",
    "#   ax.legend()\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/trainer_runtime_all.png\", format=\"png\",\n",
    "#               pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "from utils.viz_eval_utils import plot_param_count, plot_train_losses, plot_val_losses, plot_train_dices, plot_val_dices, plot_train_jaccards, plot_val_jacards, plot_trainer_runtime\n",
    "\n",
    "plot_train_losses(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_val_losses(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_train_dices(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_val_dices(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_train_jaccards(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_val_jacards(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_param_count(results_dict=train_results_dict, paletteCols=cols)\n",
    "plot_trainer_runtime(results_dict=train_results_dict, paletteCols=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz_eval_utils import plot_WT_dice, plot_WT_jaccard, plot_TC_dice, plot_TC_jaccard, plot_ET_dice, plot_ET_jaccard, plot_inference_time\n",
    "\n",
    "# def plot_WT_dice(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"WT dice\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"WT Dice Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"WT Dice\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/WT_dice_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_WT_jaccard(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"WT jaccard\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"WT Jaccard Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"WT Jaccard\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/WT_jaccard_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_TC_dice(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"TC dice\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"TC Dice Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"TC Dice\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/TC_dice_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_TC_jaccard(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"TC jaccard\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"TC Jaccard Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"TC Jaccard\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/TC_jaccard_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_ET_dice(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"ET dice\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"ET Dice Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"ET Dice\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/ET_dice_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "  \n",
    "\n",
    "# def plot_ET_jaccard(results_dict, paletteCols):\n",
    "#   mean_scores_dict = {\n",
    "#       k: results_dict[k][\"ET jaccard\"] for k in results_dict}\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for model_name in mean_scores_dict:\n",
    "#     x.append(model_name)\n",
    "#     y.append(round(mean_scores_dict[model_name],3))\n",
    "#   x = [val for _, val in sorted(zip(y, x))]\n",
    "#   y = sorted(y)\n",
    "#   fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#   ax.set_title(\"ET Jaccard Scores\")\n",
    "#   # plt.bar(x, y)\n",
    "\n",
    "#   sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#   ax.bar_label(ax.containers[0])\n",
    "\n",
    "#   ax.set_xlabel(\"Models\")\n",
    "#   ax.set_ylabel(\"ET Jaccard\")\n",
    "#   ax.set_ylim(0.5, 0.9)\n",
    "#   plt.xticks(rotation=45, ha='right')\n",
    "#   fig.savefig(\"results/ET_jaccard_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# def plot_inference_time(results_dict, paletteCols):\n",
    "#     inferenceDict = {\n",
    "#         k: results_dict[k][\"Inference time\"] for k in results_dict}\n",
    "#     # print(inferenceDict)\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for model_name in inferenceDict:\n",
    "#         x.append(model_name)\n",
    "#         y.append(inferenceDict[model_name])\n",
    "\n",
    "#     # sort based on ascending order of parameter count\n",
    "#     x = [val for _, val in sorted(zip(y, x))]\n",
    "#     y = sorted(y)\n",
    "#     y = [round(i.seconds + i.microseconds*1e-6, 3) for i in y]\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     ax.set_title(\"Inference time for 53 samples (seconds)\")\n",
    "\n",
    "#     sns.barplot(x=x, y=y, palette=paletteCols)\n",
    "#     ax.bar_label(ax.containers[0])\n",
    "#     ax.set_xlabel(\"Models\")\n",
    "#     ax.set_ylabel(\"Inference time in seconds\")\n",
    "#     ax.legend()\n",
    "#     ax.set_ylim(15, 40)\n",
    "#     plt.xticks(rotation=45, ha='right')\n",
    "#     fig.savefig(\"results/inference_time_all.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_WT_dice(eval_dict_last_epoch, cols)\n",
    "plot_WT_jaccard(eval_dict_last_epoch, cols)\n",
    "plot_TC_dice(eval_dict_last_epoch, cols)\n",
    "plot_TC_jaccard(eval_dict_last_epoch, cols)\n",
    "plot_ET_dice(eval_dict_last_epoch, cols)\n",
    "plot_ET_jaccard(eval_dict_last_epoch, cols)\n",
    "plot_inference_time(eval_dict_last_epoch, cols)\n",
    "# plot_trainer_runtime(train_results_dict, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict_last_epoch[\"3DOnet_DoubleConv_Kernel1\"]['val_metics_df']\n",
    "for k in eval_dict_last_epoch:\n",
    "  eval_dict_last_epoch[k]['val_metics_df'] = eval_dict_last_epoch[k]['val_metics_df'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for k in train_results_dict:\n",
    "  x.append(k)\n",
    "  y.append(train_results_dict[k][\"parameter_count\"])\n",
    "  # parameter_count_df.loc[k] = results_dict[k][\"parameter_count\"]\n",
    "data_tuple = list(zip(x, y))\n",
    "parameter_count_df = pd.DataFrame(\n",
    "    data_tuple, columns=['Model', 'Params (1e6)']).set_index(\"Model\")\n",
    "parameter_count_df = parameter_count_df.applymap(lambda x: round(x*1e-6,2))\n",
    "parameter_count_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = []\n",
    "y = []\n",
    "for k in train_results_dict:\n",
    "  x.append(k)\n",
    "  y.append(train_results_dict[k][\"trainer_runtime\"])\n",
    "  # parameter_count_df.loc[k] = results_dict[k][\"parameter_count\"]\n",
    "y = [round(i.hour+(i.minute/60),3) for i in y]\n",
    "data_tuple = list(zip(x, y))\n",
    "trainer_runtime_df = pd.DataFrame(\n",
    "    data_tuple, columns=['Model', 'Train time (h)']).set_index(\"Model\")\n",
    "# trainer_runtime_df = trainer_runtime_df.applymap(lambda x: round(x*1e-6, 2))\n",
    "trainer_runtime_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "# for k in eval_dict_last_epoch:\n",
    "results_df = pd.DataFrame.from_dict(eval_dict_last_epoch)\n",
    "results_df = results_df.transpose().sort_index()\n",
    "results_rounded = results_df\n",
    "results_rounded[\"Inference time\"] = results_rounded[\"Inference time\"].apply(\n",
    "    lambda x: float(np.format_float_positional(x.seconds + x.microseconds*1e-6, precision=4, unique=False, fractional=False, trim='k')))\n",
    "# results_df.applymap('{:,.2f}'.format)\n",
    "# results_rounded\n",
    "results_rounded = results_rounded.applymap(lambda x: float(\n",
    "    np.format_float_positional(x, precision=4, unique=False, fractional=False, trim='k')))\n",
    "\n",
    "results_rounded = results_rounded.join(trainer_runtime_df)\n",
    "results_rounded = results_rounded.join(parameter_count_df)\n",
    "\n",
    "results_rounded.rename(\n",
    "    columns={'Inference time': 'Infer time (s)'}, inplace=True)\n",
    "results_rounded.to_csv(\"results/full_results_statistics.csv\")\n",
    "results_rounded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_rounded.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_csv_per_model_in_log():\n",
    "    for k in eval_dict_last_epoch:\n",
    "        print(k)\n",
    "        sample_results = pd.DataFrame.from_dict(\n",
    "            eval_dict_last_epoch[k][\"val_metics_df\"])\n",
    "        sample_results.to_csv(f\"Logs/{k}/eval_results.csv\")\n",
    "        \n",
    "\n",
    "\n",
    "save_eval_csv_per_model_in_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions_in_results(target=\"BraTS20_Training_004\", treshold=0.5):\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  name, imgs, targets = None, None, None\n",
    "  _, _, test_dataloader = get_dataloaders(\n",
    "      dataset=BratsDataset, path_to_csv=\"./fold_data.csv\", val_fold=0, test_fold=1, batch_size=1, do_resizing=True)\n",
    "\n",
    "  for i, data in enumerate(test_dataloader):\n",
    "    if data['Id'][0] == target:\n",
    "      print(\"Target found: \", target)\n",
    "      name, imgs, targets = data['Id'][0], data['image'], data['mask']\n",
    "      \n",
    "  fig, ax_array = plt.subplots(len(os.listdir(\"Logs\")), 7, figsize=(20, 50))\n",
    "  \n",
    "  rows = []\n",
    "  cols = ['img', 'WT_GT', 'WT_Pred', 'TC_GT', 'TC_Pred', 'ET_GT', 'ET_Pred']\n",
    "  for i, dirname in enumerate(os.listdir(\"Logs\")):\n",
    "    \n",
    "    model_name = dirname\n",
    "    rows.append(model_name)\n",
    "    model = modelDict[model_name]\n",
    "    checkpoint_path = None\n",
    "\n",
    "    for filename in os.listdir(f\"Logs/{dirname}\"):\n",
    "      if \"your_last_epoch_model\" in filename:\n",
    "        checkpoint_path = f\"Logs/{dirname}/{filename}\"\n",
    "        break\n",
    "\n",
    "    try:\n",
    "      model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "      model.eval()\n",
    "      print(f\"{model_name} loaded with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "    except Exception as e:\n",
    "      print(\n",
    "          f\"Error loading {model_name} with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "      print(e)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      print(\"Predicting for: \", model_name)\n",
    "      imgs, targets = imgs.to(device), targets.to(device)\n",
    "      logits = model(imgs)\n",
    "      probs = torch.sigmoid(logits)\n",
    "\n",
    "      predictions = (probs >= treshold).float()\n",
    "      predictions = predictions.cpu()\n",
    "      targets = targets.cpu()\n",
    "\n",
    "      img, gt, prediction = imgs.cpu(), targets, predictions\n",
    "\n",
    "      img = img.squeeze()[0].cpu().detach().numpy()\n",
    "      gt0 = gt[0][0].squeeze().cpu().detach().numpy()\n",
    "      gt1 = gt[0][1].squeeze().cpu().detach().numpy()\n",
    "      gt2 = gt[0][2].squeeze().cpu().detach().numpy()\n",
    "      pred0 = prediction[0][0].squeeze().cpu().detach().numpy()\n",
    "      pred1 = prediction[0][1].squeeze().cpu().detach().numpy()\n",
    "      pred2 = prediction[0][2].squeeze().cpu().detach().numpy()\n",
    "\n",
    "      \n",
    "      slice_w = 75\n",
    "      \n",
    "      ax_array[i][0].imshow(img[slice_w, :, :], cmap=\"gray\")\n",
    "      ax_array[i][0].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][0].set_xlabel('')\n",
    "      ax_array[i][0].set_ylabel('')\n",
    "      ax_array[i][1].imshow(gt0[slice_w, :, :], cmap=\"viridis\")\n",
    "      ax_array[i][1].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][1].set_xlabel('')\n",
    "      ax_array[i][1].set_ylabel('')\n",
    "      ax_array[i][2].imshow(pred0[slice_w, :, :], cmap=\"viridis\")\n",
    "      ax_array[i][2].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][2].set_xlabel('')\n",
    "      ax_array[i][2].set_ylabel('')\n",
    "      ax_array[i][3].imshow(gt1[slice_w, :, :], cmap=\"cividis\")\n",
    "      ax_array[i][3].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][3].set_xlabel('')\n",
    "      ax_array[i][3].set_ylabel('')\n",
    "      ax_array[i][4].imshow(pred1[slice_w, :, :], cmap=\"cividis\")\n",
    "      ax_array[i][4].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][4].set_xlabel('')\n",
    "      ax_array[i][4].set_ylabel('')\n",
    "      ax_array[i][5].imshow(gt2[slice_w, :, :], cmap=\"plasma\")\n",
    "      ax_array[i][5].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][5].set_xlabel('')\n",
    "      ax_array[i][5].set_ylabel('')\n",
    "      ax_array[i][6].imshow(pred2[slice_w, :, :], cmap=\"plasma\")\n",
    "      ax_array[i][6].tick_params(axis='both', length=0, width=0)\n",
    "      ax_array[i][6].set_xlabel('')\n",
    "      ax_array[i][6].set_ylabel('')\n",
    "\n",
    "    del model\n",
    "  for ax, col in zip(ax_array[0], cols):\n",
    "    ax.set_title(col)\n",
    "  for ax, row in zip(ax_array[:, 0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size='small')\n",
    "  fig.savefig(f\"results/prediction_comparison.png\",\n",
    "              format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "  print(\n",
    "      f\"Saving to results/prediction_comparison.png\")\n",
    "  plt.show()\n",
    "  # fig.suptitle(f'Model: {model_name} | Sample: {name} | Slice: {slice_w}', y=0.7, fontsize=16)\n",
    "\n",
    "\n",
    "compare_predictions_in_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_logs(target=\"BraTS20_Training_004\", treshold=0.5):\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "  name, imgs, targets = None, None, None\n",
    "  _, _, test_dataloader = get_dataloaders(\n",
    "      dataset=BratsDataset, path_to_csv=\"./fold_data.csv\", val_fold=0, test_fold=1, batch_size=1, do_resizing=True)\n",
    "  \n",
    "  for i, data in enumerate(test_dataloader):\n",
    "    if data['Id'][0] == target:\n",
    "      print(\"Target found: \", target)\n",
    "      name, imgs, targets = data['Id'][0], data['image'], data['mask']\n",
    "  \n",
    "  for dirname in os.listdir(\"Logs\"):\n",
    "    model_name = dirname\n",
    "    model = modelDict[model_name]\n",
    "    savepath = f\"Logs/{dirname}/prediction_samples\"\n",
    "    if not os.path.isdir(savepath):\n",
    "      os.mkdir(savepath)\n",
    "    results = {\"Id\": [], \"image\": [], \"GT\": [], \"Prediction\": []}\n",
    "\n",
    "    checkpoint_path = None\n",
    "\n",
    "    for filename in os.listdir(f\"Logs/{dirname}\"):\n",
    "      if \"your_last_epoch_model\" in filename:\n",
    "        checkpoint_path = f\"Logs/{dirname}/{filename}\"\n",
    "        break\n",
    "\n",
    "    try:\n",
    "      model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "      model.eval()\n",
    "      print(f\"{model_name} loaded with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading {model_name} with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "      print(e)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      print(\"Predicting for: \", model_name)\n",
    "      imgs, targets = imgs.to(device), targets.to(device)\n",
    "      logits = model(imgs)\n",
    "      probs = torch.sigmoid(logits)\n",
    "\n",
    "      predictions = (probs >= treshold).float()\n",
    "      predictions = predictions.cpu()\n",
    "      targets = targets.cpu()\n",
    "      \n",
    "      img, gt, prediction = imgs.cpu(), targets, predictions\n",
    "      \n",
    "      img = img.squeeze()[0].cpu().detach().numpy()\n",
    "      gt0 = gt[0][0].squeeze().cpu().detach().numpy()\n",
    "      gt1 = gt[0][1].squeeze().cpu().detach().numpy()\n",
    "      gt2 = gt[0][2].squeeze().cpu().detach().numpy()\n",
    "      pred0 = prediction[0][0].squeeze().cpu().detach().numpy()\n",
    "      pred1 = prediction[0][1].squeeze().cpu().detach().numpy()\n",
    "      pred2 = prediction[0][2].squeeze().cpu().detach().numpy()\n",
    "      \n",
    "      for i in range(50, 90+1, 5):\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(\n",
    "            1, 7, figsize=(30, 10))\n",
    "        slice_w = i\n",
    "        \n",
    "        ax1.imshow(img[slice_w, :, :], cmap=\"gray\")\n",
    "        ax1.set_title(f'img')\n",
    "        ax2.imshow(gt0[slice_w, :, :], cmap=\"viridis\")\n",
    "        ax2.set_title(f'WT_GT')\n",
    "        ax3.imshow(pred0[slice_w, :, :], cmap=\"viridis\")\n",
    "        ax3.set_title(f'WT_Pred')\n",
    "        ax4.imshow(gt1[slice_w, :, :], cmap=\"cividis\")\n",
    "        ax4.set_title(f'TC_GT')\n",
    "        ax5.imshow(pred1[slice_w, :, :], cmap=\"cividis\")\n",
    "        ax5.set_title(f'TC_Pred')\n",
    "        ax6.imshow(gt2[slice_w, :, :], cmap=\"plasma\")\n",
    "        ax6.set_title(f'ET_GT')\n",
    "        ax7.imshow(pred2[slice_w, :, :], cmap=\"plasma\")\n",
    "        ax7.set_title(f'ET_Pred')\n",
    "        # fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        fig.suptitle(\n",
    "            f'Model: {model_name} | Sample: {name} | Slice: {i}', y=0.7, fontsize=16)\n",
    "        # plt.show()\n",
    "        fig.savefig(f\"{savepath}/prediction_{name}_slice_{i}.png\",\n",
    "                    format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "        print(\n",
    "            f\"Saving to {savepath}/prediction_{name}_slice_{i}.png\")\n",
    "    del model\n",
    "  \n",
    "\n",
    "save_predictions_to_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
