# 3D Brain Tumour Segmentation Project

By Bryan Tan (Chen Zhengyu), Christy Lau Jin Yun and Lee Pei Xuan.

# Introduction

Brain Tumour Segmentation is an essential challenge in biomedical image processing. Though tumour segmentation is traditionally done manually by doctors worldwide, it can be done very quickly with the help of 3-dimensional image processing and computer vision. Such methods can facilitate the early detection of brain tumours which is vital for the successful treatment of patients. One of the most prevalent types of primary brain tumours is Giloma, with a survival rate of just over 6.8% and which comprises about 30 percent of brain tumours and 80 percent of all malignant brain tumours. Automated brain-tumour segmentation approaches can hence help reduce hospital workloads and save lives.

## Requirements

- **Libraries used**: torch, torchvision, torchviz, nilearn, numpy, pandas, matplotlib, seaborn, skimage etc. Tested on torch==1.13.1.

- **Device specifications**: Intel i5-13600k with 32GB of RAM and an RTX 3090 with 24GB VRAM.

- Training and evaluation are done on Ubuntu-22.04 via WSL.

## Instructions

### Setup

1. Clone [this repository](https://github.com/Inc0mple/3D_Brain_Tumor_Seg_V2).

1. Download this [BraTS2020 dataset](https://www.kaggle.com/datasets/overspleen/brats-2020-fixed-355) from Kaggle into the repository folder.

### Notable files and folders

- **`VizEval_Single_Notebook.ipynb`** contains the code necessary to evaluate and visualise the predictions of a single model.

- **`VizEval_Notebook.ipynb`** contains the code necessary to evaluate all models and save the results to their respective `./Logs` folders and the `./results` folder.

- **`Train_Notebook.ipynb`** contains the code necessary to train a defined model and save the logs in the `./Logs` folder.

- **`./Logs`** consists of folders containing the training and evaluation logs for each model, as well as sample predictions. It also contains model checkpoints, images of the model structure and `trainer_properties.txt` which stores the model definition and tracked losses, among useful information. The folder is populated by code from `Train_Notebook.ipynb` and `VizEval_Notebook.ipynb`.

- **`./Images`** contains miscellaneous images that are used in the README or the report.

- **`./models`** contain `.py` files of PyTorch models which are imported and instantiated for use in `Train_Notebook.ipynb` and `VizEval_Notebook.ipynb.`

- **`./results`** contain aggregate statistics/visualisations for all models.

- **`fold_data.csv`** is generated by `Train_Notebook.ipynb` and contains the information to map different patients (and their directory paths) to different training/validation/testing folds.

- **`./utils`** contain vital functions and classes such as `BratsDataset.py` (the dataset class), `Meter.py` (class for tracking results in the `Trainer` class) as well as other utility functions for visualisation/evaluations.

- **`Misc_Visualisations.ipynb`** is a workspace used to generate miscellaneous visualisations which may be included in the final report.

- **`fold_data.csv`** contains the information for mapping different patients (and their directory paths) to different training folds; generated from `Train_Notebook.ipynb`.

### Training

1. Design/Modify your model in the `models` folder.

2. Import and instantiate your model in the 2nd cell of `Train_Notebook.ipynb`

3. In the 3rd cell of `Train_Notebook.ipynb`, assign `"./Log/{your_model_name}"` to `train_logs_path` ; training results will be saved to this folder.

4. Run all cells in `Train_Notebook.ipynb` and wait till training is complete.

5. To evaluate all models, go to the 4th cell in `VizEval_Notebook.ipynb` and populate `modelDict` with the model names and its corresponding instantiations. The model name should be exactly the same as the name of its folder in `./Logs`.

6. Run all cells in `VizEval_Notebook.ipynb` to populate the results folder with aggregate statistics/visualisations for all models and add additional evaluations/visualisations to the `./Logs` folder.

### Evaluation

1. To evaluate a single cell, run all cells in `VizEval_Single_Notebook.ipynb`, selecting one of the available models with a number when prompted for user input.

2. To evaluate all models, go to the 4th cell in `VizEval_Notebook.ipynb` and populate `modelDict` with the model names and its corresponding instantiations. The model name should be exactly the same as the name of its folder in `./Logs`.

3. Run all cells in `VizEval_Notebook.ipynb` to populate the results folder with aggregate statistics/visualisations for all models and add additional evaluations/visualisations to the `./Logs` folder.



### Useful References
- [**BraTS_2020 Dataset on Kaggle**](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation) (More details on Dataset in Data Description section of [this paper](https://arxiv.org/pdf/2011.02881.pdf#:~:text=The%20BraTS%202020%20training%20dataset,and%201%20mm%20isotropic%20resolution.))
- [BraTS_2021 Dataset on Kaggle (Superset of BraTS_2020)](https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1)
- [**Full PyTorch Implementation 3D_UNet on BraTS_2020 Dataset**](https://www.kaggle.com/code/polomarco/brats20-3dunet-3dautoencoder)
- [**Tensorflow Implementation of 3D UNet on BraTS_2020 Dataset**](https://www.kaggle.com/code/maksudaislamlima/3d-unet-brats2020) (Still useful reference for designing our model/dataset/dataloader classes despite not being PyTorch)
- [**3D-UNet BraTS Pytorch Github**](https://github.com/pheonix-18/3D-Unet-BraTS-PyTorch)
- [U-Net on Wikipedia](https://en.wikipedia.org/wiki/U-Net)
- [Paper on 3D U-Net](https://arxiv.org/pdf/1606.06650.pdf)
- [Unofficial 3DUnet PyTorch implementation on Github](https://github.com/AghdamAmir/3D-UNet/blob/main/unet3d.py)
- [Vision Transformers on Wikipedia](https://en.wikipedia.org/wiki/Vision_transformer) (Harder to optimise and train; may require large datasets; but potentially better results than pure CNNs)
- [Explanation on Swin Transformer](https://towardsdatascience.com/a-comprehensive-guide-to-swin-transformer-64965f89d14c); a more efficient version of Vision Transformers
- [Heavily Abstracted notebook on training a SwinUNETR model with BraTS Dataset using Project MONAI](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb)

### Performance Comparisons
- [**Performance of different UNET models on Synapse multi-organ CT**](https://paperswithcode.com/sota/medical-image-segmentation-on-synapse-multi?p=swin-unet-unet-like-pure-transformer-for)
- [Nvidiaâ€™s UNet Models used for the BraTS 2021 Dataset](https://developer.nvidia.com/blog/nvidia-data-scientists-take-top-spots-in-miccai-2021-brain-tumor-segmentation-challenge/)

### Architectures to Experiment With
- [SwinUNETR](https://arxiv.org/abs/2201.01266)