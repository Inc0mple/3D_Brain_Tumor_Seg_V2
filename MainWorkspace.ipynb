{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data downloaded from [here](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation)\n","\n","All BraTS multimodal scans are available as NIfTI files (.nii.gz) and describe a) native (T1) and b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes.\n","\n","Annotations comprise the GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor core (NCR/NET — label 1).\n","\n","# IMPORTANT: Apparently training data 355's segmentation file is named incorrectly in the original dataset; manual renaming to the correct naming convention is required to prevent errors \n","### Alternatively, skip 355 by uncommenting the appropriate line of code under the \"Preprocessing\" section\n","\n","### Code referenced from [this kaggle notebook](https://www.kaggle.com/code/polomarco/brats20-3dunet-3dautoencoder/data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:03.099485Z","iopub.status.busy":"2023-02-25T14:22:03.099128Z","iopub.status.idle":"2023-02-25T14:22:06.945961Z","shell.execute_reply":"2023-02-25T14:22:06.944942Z","shell.execute_reply.started":"2023-02-25T14:22:03.099454Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import time\n","from datetime import datetime\n","from random import randint\n","\n","import numpy as np\n","from scipy import stats\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVR\n","from sklearn.model_selection import KFold\n","\n","import nibabel as nib\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import matplotlib.animation as anim\n","import matplotlib.patches as mpatches\n","import matplotlib.gridspec as gridspec\n","\n","import seaborn as sns\n","from skimage.transform import resize\n","from skimage.util import montage\n","\n","from IPython.display import Image as show_gif\n","from IPython.display import clear_output\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam, AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","\n","import warnings\n","warnings.simplefilter(\"ignore\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# DEFINE YOUR MODEL HERE\n","\n","## Go to models.UNet3d_your_modifications and modify 3DUnet model to match performance of SwinUNETR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils.Meter import Meter, DiceLoss, BCEDiceLoss, dice_coef_metric_per_classes, jaccard_coef_metric_per_classes\n","\n","# CAN POTENTIALLY TRY DIFFERENT AUGMENTATION TECHNIQUES HERE\n","from utils.BratsDataset import BratsDataset\n","\n","from utils.Meter import BCEDiceLoss\n","\n","from models.UNet3d import UNet3d\n","from models.UNet3dDropout import UNet3dDropout\n","from models.SwinUNETR import SwinUNETR\n","from models.UNet3d_your_modifications import UNet3d_your_modifications\n","\n","\n","# model = UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')\n","model = UNet3d_your_modifications(\n","    in_channels=4, n_classes=3, n_channels=24).to('cuda')  # n_channels should be in multiples of num_groups (default 8)\n","# model = SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2,4,8,16)).to('cuda')\n","\n","\n","\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Global config class\n","class GlobalConfig:\n","    # Global config class\n","    root_dir = './BraTS2020'\n","    train_root_dir = './BraTS2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n","    test_root_dir = './BraTS2020/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n","    # CSV where patient ID, path and fold information will be stored\n","    path_to_csv = './fold_data.csv'\n","    # Where your pretrained model is stored (Specifically for the 3DUnet architecture implemented here)\n","    pretrained_model_path = './BraTS2020Logs/model_by_original_notebook_author.pth'\n","    # Where train log is stored; each row (epoch) consists of train/val loss, dice score and jaccard score.  \n","    # TO DO: Change this for changes in different hyperparam/Model\n","    train_logs_path = './BraTS2020Logs3DUnet_BCEDice_Cropped_Fixed_your_modifications'\n","    # for reproducibility\n","    seed = 42\n","\n","\n","def seed_everything(seed: int):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","\n","def load_img(file_path):\n","    data = nib.load(file_path)\n","    data = np.asarray(data.dataobj)\n","    return data\n","\n","config = GlobalConfig()\n","seed_everything(config.seed)\n","if not os.path.isdir(config.train_logs_path):\n","    os.mkdir(config.train_logs_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Visualisations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:06.950703Z","iopub.status.busy":"2023-02-25T14:22:06.950307Z","iopub.status.idle":"2023-02-25T14:22:06.995245Z","shell.execute_reply":"2023-02-25T14:22:06.994095Z","shell.execute_reply.started":"2023-02-25T14:22:06.950669Z"},"trusted":true},"outputs":[],"source":["# Change these to visualise brain MRIs of different patients, and at different slices \n","PATIENT_IDX = 10 # 1 - 369\n","SLICE_IDX = 85 # 0 - 154\n","\n","\n","sample_filename = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_flair.nii\"\n","sample_filename_mask = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_seg.nii\"\n","\n","sample_img = load_img(sample_filename)\n","sample_mask = load_img(sample_filename_mask)\n","\n","sample_filename2 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t1.nii\"\n","sample_img2 = load_img(sample_filename2)\n","\n","sample_filename3 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t2.nii\"\n","sample_img3 = load_img(sample_filename3)\n","\n","sample_filename4 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t1ce.nii\"\n","sample_img4 = load_img(sample_filename4)\n","\n","# (240, 240, 155), corresponding to height, width and depth\n","print(\"img shape ->\", sample_img.shape)  \n","print(\"mask shape ->\", sample_mask.shape) \n","# [0,1,2,4], corresponding to the background (BG — label 0), necrotic and non-enhancing tumor core (NCR/NET — label 1), the peritumoral edema (ED — label 2) and GD-enhancing tumor (ET — label 4)\n","print(\"mask unique ->\", np.unique(sample_mask)) \n","\n","# In the BraTS challenge, the segmentation performance is evaluated on three partially overlapping sub-regions of tumors,\n","# namely, whole tumor (WT), tumor core (TC), and enhancing tumor (ET).\n","# The WT is the union of ED, NCR/NET, and ET, while the TC includes NCR/NET and ET.\n","# We hence create 3 sets of mask. This will eventually be stacked upon each other during preprocessing in the Dataset Class.\n","mask_WT = sample_mask.copy()\n","mask_WT[mask_WT == 1] = 1\n","mask_WT[mask_WT == 2] = 1\n","mask_WT[mask_WT == 4] = 1\n","\n","mask_TC = sample_mask.copy()\n","mask_TC[mask_TC == 1] = 1\n","mask_TC[mask_TC == 2] = 0\n","mask_TC[mask_TC == 4] = 1\n","\n","mask_ET = sample_mask.copy()\n","mask_ET[mask_ET == 1] = 0\n","mask_ET[mask_ET == 2] = 0\n","mask_ET[mask_ET == 4] = 1\n","\n","\n","# https://matplotlib.org/3.3.2/gallery/images_contours_and_fields/plot_streamplot.html#sphx-glr-gallery-images-contours-and-fields-plot-streamplot-py\n","# https://stackoverflow.com/questions/25482876/how-to-add-legend-to-imshow-in-matplotlib\n","\n","\n","fig = plt.figure(figsize=(20, 10))\n","\n","gs = gridspec.GridSpec(nrows=2, ncols=4, height_ratios=[1, 1.5])\n","\n","#  Varying density along a streamline\n","ax0 = fig.add_subplot(gs[0, 0])\n","flair = ax0.imshow(sample_img[:, :, SLICE_IDX], cmap='bone') # Show all columns and rows, but a specific slice index\n","ax0.set_title(\"FLAIR\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(flair)\n","\n","#  Varying density along a streamline\n","ax1 = fig.add_subplot(gs[0, 1])\n","t1 = ax1.imshow(sample_img2[:, :, SLICE_IDX], cmap='bone')\n","ax1.set_title(\"T1\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t1)\n","\n","#  Varying density along a streamline\n","ax2 = fig.add_subplot(gs[0, 2])\n","t2 = ax2.imshow(sample_img3[:, :, SLICE_IDX], cmap='bone')\n","ax2.set_title(\"T2\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t2)\n","\n","#  Varying density along a streamline\n","ax3 = fig.add_subplot(gs[0, 3])\n","t1ce = ax3.imshow(sample_img4[:, :, SLICE_IDX], cmap='bone')\n","ax3.set_title(\"T1 contrast\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t1ce)\n","\n","#  Varying density along a streamline\n","ax4 = fig.add_subplot(gs[1, 1:3])\n","\n","#ax4.imshow(np.ma.masked_where(mask_WT[:,:,SLICE_IDX]== False,  mask_WT[:,:,SLICE_IDX]), cmap='summer', alpha=0.6)\n","l1 = ax4.imshow(mask_WT[:, :, SLICE_IDX], cmap='summer',)\n","l2 = ax4.imshow(np.ma.masked_where(mask_TC[:, :, SLICE_IDX] == False,  mask_TC[:, :, SLICE_IDX]), cmap='rainbow', alpha=0.6)\n","l3 = ax4.imshow(np.ma.masked_where(mask_ET[:, :, SLICE_IDX] == False, mask_ET[:, :, SLICE_IDX]), cmap='winter', alpha=0.6)\n","\n","ax4.set_title(\"\", fontsize=20, weight='bold', y=-0.1)\n","\n","_ = [ax.set_axis_off() for ax in [ax0, ax1, ax2, ax3, ax4]]\n","\n","colors = [im.cmap(im.norm(1)) for im in [l1, l2, l3]]\n","labels = ['Non-Enhancing tumor core',\n","          'Peritumoral Edema ', 'GD-enhancing tumor']\n","patches = [mpatches.Patch(\n","    color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n","# put those patched as legend-handles into the legend\n","plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4, fontsize='xx-large',\n","           title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n","\n","plt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\",\n","             fontsize=20, weight='bold')\n","\n","# fig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2,\n","#             transparent=False, bbox_inches='tight')\n","# fig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2,\n","#             transparent=False, bbox_inches='tight')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Preprocessing\n","\n","### Mapping different patients (and the paths to their data) to different training folds; Fold 0 will be used as validation set; rest will be used as training set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:08.769292Z","iopub.status.busy":"2023-02-25T14:22:08.768899Z","iopub.status.idle":"2023-02-25T14:22:09.238336Z","shell.execute_reply":"2023-02-25T14:22:09.237426Z","shell.execute_reply.started":"2023-02-25T14:22:08.769252Z"},"trusted":true},"outputs":[],"source":["survivalInfoPath = f\"{config.train_root_dir}/survival_info.csv\"\n","nameMappingPath = f\"{config.train_root_dir}/name_mapping.csv\"\n","survival_info_df = pd.read_csv(survivalInfoPath)\n","name_mapping_df = pd.read_csv(nameMappingPath)\n","\n","name_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n","\n","\n","df = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n","df = df[[\"Brats20ID\"]]\n","paths = []\n","for _, row  in df.iterrows():\n","    \n","    id_ = row['Brats20ID']\n","    path = os.path.join(config.train_root_dir, id_)\n","    paths.append(path)\n","    \n","df['path'] = paths\n","\n","train_data = df\n","\n","\"\"\"Uncomment the following if skipping number 355 due to errors\"\"\"\n","# train_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n","\n","kf = KFold(n_splits=7, random_state=config.seed, shuffle=True)\n","for i, (train_index, val_index) in enumerate(kf.split(train_data)):\n","    # assign all rows at val_index to the ith fold\n","    train_data.loc[val_index, \"fold\"] = i\n","\n","\n","train_data.to_csv(config.path_to_csv, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.286304Z","iopub.status.busy":"2023-02-25T14:22:09.285770Z","iopub.status.idle":"2023-02-25T14:22:09.297705Z","shell.execute_reply":"2023-02-25T14:22:09.296767Z","shell.execute_reply.started":"2023-02-25T14:22:09.286267Z"},"trusted":true},"outputs":[],"source":["def get_dataloaders(\n","    dataset: torch.utils.data.Dataset,\n","    path_to_csv: str,\n","    # phase: str,\n","    val_fold: int = 0, # Choose which fold to be the validation fold\n","    test_fold: int = 1,\n","    batch_size: int = 1,\n","    num_workers: int = 4,\n","    do_resizing: bool = True,\n","):\n","    assert(val_fold != test_fold)\n","    \n","    df = pd.read_csv(path_to_csv)\n","    \n","    '''Returns: dataloader for the model training'''\n","    # Data in folds other than 0 are used for training\n","    train_df = df.loc[~df['fold'].isin([val_fold, test_fold])].reset_index(drop=True)\n","    # Data in fold 0 is used for validation\n","    val_df = df.loc[df['fold'] == val_fold].reset_index(drop=True)\n","    test_df = df.loc[df['fold'] == test_fold].reset_index(drop=True)\n","    \n","    # dataset = dataset(df, phase)\n","    train_dataset = dataset(train_df, \"train\", do_resizing=do_resizing)\n","    val_dataset = dataset(val_df, \"val\", do_resizing=do_resizing)\n","    test_dataset = dataset(test_df, \"test\", do_resizing=do_resizing)\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    test_dataloader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    return train_dataloader, val_dataloader, test_dataloader"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:24.970570Z","iopub.status.busy":"2023-02-25T14:22:24.970150Z","iopub.status.idle":"2023-02-25T14:22:25.005465Z","shell.execute_reply":"2023-02-25T14:22:25.004123Z","shell.execute_reply.started":"2023-02-25T14:22:24.970535Z"},"trusted":true},"outputs":[],"source":["class Trainer:\n","    \"\"\"\n","    Factory for training proccess.\n","    Args:\n","        display_plot: if True - plot train history after each epoch.\n","        net: neural network for mask prediction.\n","        criterion: factory for calculating objective loss.\n","        optimizer: optimizer for weights updating.\n","        phases: list with train and validation phases.\n","        dataloaders: dict with data loaders for train and val phases.\n","        path_to_csv: path to csv file.\n","        meter: factory for storing and updating metrics.\n","        batch_size: data batch size for one step weights updating.\n","        num_epochs: num weights updation for all data.\n","        accumulation_steps: the number of steps after which the optimization step can be taken\n","                    (https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614).\n","        lr: learning rate for optimizer.\n","        scheduler: scheduler for control learning rate.\n","        losses: dict for storing lists with losses for each phase.\n","        jaccard_scores: dict for storing lists with jaccard scores for each phase.\n","        dice_scores: dict for storing lists with dice scores for each phase.\n","    \"\"\"\n","    def __init__(self,\n","                 net: nn.Module,\n","                 dataset: torch.utils.data.Dataset,\n","                 criterion: nn.Module,\n","                 lr: float,\n","                 accumulation_steps: int,\n","                 batch_size: int,\n","                 val_fold: int,\n","                 test_fold: int,\n","                 num_epochs: int,\n","                 path_to_csv: str,\n","                 display_plot: bool = True,\n","                 do_resizing: bool = True,\n","                 optimizer: torch.optim = Adam\n","                ):\n","\n","        \"\"\"Initialization.\"\"\"\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        print(\"device:\", self.device)\n","        self.display_plot = display_plot\n","        self.net = net\n","        self.net = self.net.to(self.device)\n","        self.criterion = criterion\n","        self.optimizer = optimizer(self.net.parameters(), lr=lr)\n","        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\",  # Reduces learning rate when a metric has stopped improving\n","                                           patience=2, verbose=True)\n","        self.accumulation_steps = accumulation_steps // batch_size\n","        self.phases = [\"train\", \"val\"]\n","        self.num_epochs = num_epochs\n","        train_dl, val_dl, test_dl = get_dataloaders(\n","            dataset=dataset,\n","            path_to_csv=path_to_csv,\n","            val_fold=val_fold,\n","            test_fold=test_fold,\n","            batch_size=batch_size,\n","            num_workers=4,\n","            do_resizing=do_resizing,\n","        )\n","        self.dataloaders = {\n","            \"train\": train_dl,\n","            \"val\": val_dl,\n","            \"test\": test_dl\n","        }\n","        self.best_loss = float(\"inf\")\n","        self.losses = {phase: [] for phase in self.phases}\n","        self.dice_scores = {phase: [] for phase in self.phases}\n","        self.jaccard_scores = {phase: [] for phase in self.phases}\n","        self.last_completed_run_time = None\n","         \n","    def _compute_loss_and_outputs(self,\n","                                  images: torch.Tensor,\n","                                  targets: torch.Tensor):\n","        images = images.to(self.device)\n","        targets = targets.to(self.device)\n","        logits = self.net(images)\n","        loss = self.criterion(logits, targets)\n","        return loss, logits\n","        \n","    def _do_epoch(self, epoch: int, phase: str):\n","        # print(f\"{phase} epoch: {epoch + 1} | time: {time.strftime('%H:%M:%S')}\")\n","\n","        self.net.train() if phase == \"train\" else self.net.eval()\n","        meter = Meter()\n","        dataloader = self.dataloaders[phase]\n","        total_batches = len(dataloader)\n","        running_loss = 0.0\n","        self.optimizer.zero_grad()\n","        t_dataloader = tqdm(enumerate(dataloader), unit=\"batch\", total=total_batches)\n","        for itr, data_batch in t_dataloader:\n","            t_dataloader.set_description(f\"{phase} epoch: {epoch + 1} | time: {time.strftime('%H:%M:%S')}\")\n","            images, targets = data_batch['image'], data_batch['mask']\n","            loss, logits = self._compute_loss_and_outputs(images, targets)\n","            loss = loss / self.accumulation_steps\n","            t_dataloader.set_postfix(loss=loss.item())\n","            if phase == \"train\":\n","                loss.backward()\n","                if (itr + 1) % self.accumulation_steps == 0:\n","                    self.optimizer.step()\n","                    self.optimizer.zero_grad()\n","            running_loss += loss.item()\n","            meter.update(logits.detach().cpu(),\n","                         targets.detach().cpu()\n","                        )\n","            \n","        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n","        epoch_dice, epoch_iou = meter.get_metrics()\n","        \n","        self.losses[phase].append(epoch_loss)\n","        self.dice_scores[phase].append(epoch_dice)\n","        self.jaccard_scores[phase].append(epoch_iou)\n","\n","        return epoch_loss\n","        \n","    def run(self):\n","        start = datetime.now()\n","        for epoch in range(self.num_epochs):\n","            self._do_epoch(epoch, \"train\")\n","            with torch.no_grad():\n","                val_loss = self._do_epoch(epoch, \"val\")\n","                self.scheduler.step(val_loss)\n","            if self.display_plot:\n","                self._plot_train_history()\n","                \n","            if val_loss < self.best_loss:\n","                print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n","                self.best_loss = val_loss\n","                now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","                checkpoint_filename = f\"your_best_model_{now}.pth\"\n","                torch.save(self.net.state_dict(), os.path.join(\n","                    config.train_logs_path, checkpoint_filename))\n","            print()\n","        self.last_completed_run_time = str(datetime.now() - start)\n","        self._save_train_history()\n","            \n","    def _plot_train_history(self):\n","        data = [self.losses, self.dice_scores, self.jaccard_scores]\n","        colors = ['deepskyblue', \"crimson\"]\n","        labels = [\n","            f\"\"\"\n","            train loss {self.losses['train'][-1]}\n","            val loss {self.losses['val'][-1]}\n","            \"\"\",\n","            \n","            f\"\"\"\n","            train dice score {self.dice_scores['train'][-1]}\n","            val dice score {self.dice_scores['val'][-1]} \n","            \"\"\", \n","                  \n","            f\"\"\"\n","            train jaccard score {self.jaccard_scores['train'][-1]}\n","            val jaccard score {self.jaccard_scores['val'][-1]}\n","            \"\"\",\n","        ]\n","        \n","        clear_output(True)\n","        with plt.style.context(\"seaborn-dark-palette\"):\n","            fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n","            for i, ax in enumerate(axes):\n","                ax.plot(data[i]['val'], c=colors[0], label=\"val\")\n","                ax.plot(data[i]['train'], c=colors[-1], label=\"train\")\n","                ax.set_title(labels[i])\n","                ax.legend(loc=\"upper right\")\n","                \n","            plt.tight_layout()\n","            plt.show()\n","            \n","    def load_predtrain_model(self,\n","                             state_path: str):\n","        self.net.load_state_dict(torch.load(state_path))\n","        print(\"Predtrain model loaded\")\n","        \n","    def _save_train_history(self):\n","        \"\"\"writing model weights and training logs to files.\"\"\"\n","        now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","        checkpoint_filename = f\"your_last_epoch_model_{now}.pth\"\n","        torch.save(self.net.state_dict(),os.path.join(\n","            config.train_logs_path, checkpoint_filename))\n","\n","        logs_ = [self.losses, self.dice_scores, self.jaccard_scores]\n","        log_names_ = [\"_loss\", \"_dice\", \"_jaccard\"]\n","        logs = [logs_[i][key] for i in list(range(len(logs_)))\n","                         for key in logs_[i]]\n","        log_names = [key+log_names_[i] \n","                     for i in list(range(len(logs_))) \n","                     for key in logs_[i]\n","                    ]\n","        # train_logs_path = './BraTS2020Logs/train_log.csv'\n","        pd.DataFrame(\n","            dict(zip(log_names, logs))\n","        ).to_csv(os.path.join(config.train_logs_path, f\"train_log_{now}.csv\"), index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trainer = Trainer(net=model,\n","#                   dataset=BratsDataset,\n","#                   criterion=BCEDiceLoss(),\n","#                   lr=5e-4,\n","#                   accumulation_steps=4,\n","#                   batch_size=1,\n","#                   val_fold=0,\n","#                   test_fold=1,\n","#                   num_epochs=50,\n","#                   path_to_csv=config.path_to_csv,)\n","DO_RESIZING = True\n","\n","trainer = Trainer(net=model,\n","                  dataset=BratsDataset,\n","                  criterion=BCEDiceLoss(),\n","                  lr=5e-4,\n","                  accumulation_steps=4,\n","                  batch_size=1,\n","                  val_fold=0,\n","                  test_fold=1,\n","                  num_epochs=50,\n","                  path_to_csv=config.path_to_csv,\n","                  do_resizing=DO_RESIZING,\n","                  optimizer=Adam)\n","\"\"\"UNCOMMENT THE FOLLOWING 2 LINES IF RELOADING MODEL CHECKPOINT\"\"\"\n","# if config.pretrained_model_path is not None:\n","#     trainer.load_predtrain_model(config.pretrained_model_path)\n","\n","# if need - load the logs.\n","# train_logs = pd.read_csv(config.train_logs_path)\n","# trainer.losses[\"train\"] =  train_logs.loc[:, \"train_loss\"].to_list()\n","# trainer.losses[\"val\"] =  train_logs.loc[:, \"val_loss\"].to_list()\n","# trainer.dice_scores[\"train\"] = train_logs.loc[:, \"train_dice\"].to_list()\n","# trainer.dice_scores[\"val\"] = train_logs.loc[:, \"val_dice\"].to_list()\n","# trainer.jaccard_scores[\"train\"] = train_logs.loc[:, \"train_jaccard\"].to_list()\n","# trainer.jaccard_scores[\"val\"] = train_logs.loc[:, \"val_jaccard\"].to_list()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.299558Z","iopub.status.busy":"2023-02-25T14:22:09.299136Z","iopub.status.idle":"2023-02-25T14:22:09.326925Z","shell.execute_reply":"2023-02-25T14:22:09.323871Z","shell.execute_reply.started":"2023-02-25T14:22:09.299518Z"},"trusted":true},"outputs":[],"source":["# dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase='valid', fold=0, batch_size=1)\n","dataloader, _, test_dataloader = get_dataloaders(\n","    dataset=BratsDataset, path_to_csv=config.path_to_csv, val_fold=0, test_fold=1, batch_size=1, do_resizing=DO_RESIZING)\n","len(dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.331159Z","iopub.status.busy":"2023-02-25T14:22:09.328642Z","iopub.status.idle":"2023-02-25T14:22:22.153941Z","shell.execute_reply":"2023-02-25T14:22:22.152948Z","shell.execute_reply.started":"2023-02-25T14:22:09.331107Z"},"trusted":true},"outputs":[],"source":["data = next(iter(dataloader))\n","data['Id'], data['image'].shape, data['mask'].shape\n","# size = (batch_size, channels, depth, width, height)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:22.155861Z","iopub.status.busy":"2023-02-25T14:22:22.155573Z","iopub.status.idle":"2023-02-25T14:22:24.747368Z","shell.execute_reply":"2023-02-25T14:22:24.746100Z","shell.execute_reply.started":"2023-02-25T14:22:22.155830Z"},"trusted":true},"outputs":[],"source":["img_tensor = data['image'].squeeze()[0].cpu().detach().numpy() \n","mask_tensor = data['mask'].squeeze()[0].squeeze().cpu().detach().numpy()\n","print(\"Num uniq Image values :\", len(np.unique(img_tensor, return_counts=True)[0]))\n","print(\"Min/Max Image values:\", img_tensor.min(), img_tensor.max())\n","print(\"Num uniq Mask values:\", np.unique(mask_tensor, return_counts=True))\n","print(img_tensor.shape)\n","image = np.rot90(montage(img_tensor))\n","mask = np.rot90(montage(mask_tensor)) \n","\n","fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n","ax.imshow(image, cmap ='bone')\n","ax.imshow(np.ma.masked_where(mask == False, mask),\n","           cmap='cool', alpha=0.6)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","## TO-DO: Train your own model; experiment with different hyperparameters\n","\n","### **Optional**: If you don't want to train from scratch, download the 3DUnet model checkpoint trained by the author of the [referenced notebook](https://www.kaggle.com/code/polomarco/brats20-3dunet-3dautoencoder/notebook) and place it in the same path as `config.pretrained_model_path`\n","\n","### Alternatively, train your own model and reload it from the same place."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"Uncomment these to clear GPU VRAM\"\"\"\n","# del model\n","torch.cuda.empty_cache()\n","torch.cuda.synchronize()\n","# assert(0==1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:24:20.715410Z","iopub.status.busy":"2023-02-25T14:24:20.714916Z"},"trusted":true},"outputs":[],"source":["# assert(0==1)\n","\"\"\"UNCOMMENT THIS TO START TRAINING\"\"\"\n","trainer.run()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# write logs to train_logs_path\n","with open(os.path.join(config.train_logs_path, 'trainer_properties.txt'), 'w') as f:\n","  for param, value in trainer.__dict__.items():\n","      if not param.startswith(\"__\"):\n","        f.write(f\"{param}:{value}\\n\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Evaluation and Results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.214845Z","iopub.status.idle":"2023-02-25T14:22:25.215649Z"},"trusted":true},"outputs":[],"source":["def compute_scores_per_classes(model,\n","                               dataloader,\n","                               classes):\n","    \"\"\"\n","    Compute Dice and Jaccard coefficients for each class.\n","    Params:\n","        model: neural net for make predictions.\n","        dataloader: dataset object to load data from.\n","        classes: list with classes.\n","        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n","    \"\"\"\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    dice_scores_per_classes = {key: list() for key in classes}\n","    iou_scores_per_classes = {key: list() for key in classes}\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader):\n","            imgs, targets = data['image'], data['mask']\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            logits = model(imgs)\n","            logits = logits.detach().cpu().numpy()\n","            targets = targets.detach().cpu().numpy()\n","            \n","            dice_scores = dice_coef_metric_per_classes(logits, targets)\n","            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n","\n","            for key in dice_scores.keys():\n","                dice_scores_per_classes[key].extend(dice_scores[key])\n","\n","            for key in iou_scores.keys():\n","                iou_scores_per_classes[key].extend(iou_scores[key])\n","\n","    return dice_scores_per_classes, iou_scores_per_classes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.216921Z","iopub.status.idle":"2023-02-25T14:22:25.217729Z"},"trusted":true},"outputs":[],"source":["# test_dataloader = get_dataloaders(dataset=BratsDataset, path_to_csv=config.path_to_csv, val_fold=0, test_fold=1, batch_size=1)\n","len(dataloader)\n","# model.load_state_dict(torch.load(\n","#     'BraTS2020Logs3DUnet_BCEDice_Cropped_Fixed/your_last_epoch_model_20230309-233020.pth', map_location='cpu'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.219050Z","iopub.status.idle":"2023-02-25T14:22:25.219967Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.221207Z","iopub.status.idle":"2023-02-25T14:22:25.221974Z"},"trusted":true},"outputs":[],"source":["# %%time\n","dice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n","    model, test_dataloader, ['WT', 'TC', 'ET']\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.223210Z","iopub.status.idle":"2023-02-25T14:22:25.224003Z"},"trusted":true},"outputs":[],"source":["dice_df = pd.DataFrame(dice_scores_per_classes)\n","dice_df.columns = ['WT dice', 'TC dice', 'ET dice']\n","\n","iou_df = pd.DataFrame(iou_scores_per_classes)\n","iou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard']\n","val_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\n","val_metics_df = val_metics_df.loc[:, ['WT dice', 'WT jaccard', \n","                                      'TC dice', 'TC jaccard', \n","                                      'ET dice', 'ET jaccard']]\n","val_metics_df.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_metics_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.225243Z","iopub.status.idle":"2023-02-25T14:22:25.226040Z"},"trusted":true},"outputs":[],"source":["colors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF']\n","palette = sns.color_palette(colors, 6)\n","\n","fig, ax = plt.subplots(figsize=(12, 6));\n","sns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\n","ax.set_xticklabels(val_metics_df.columns, fontsize=14, rotation=15);\n","ax.set_title(\"Dice and Jaccard Coefficients from Test Set\", fontsize=20)\n","\n","for idx, p in enumerate(ax.patches):\n","        percentage = '{:.1f}%'.format(100 * val_metics_df.mean().values[idx])\n","        x = p.get_x() + p.get_width() / 2 - 0.15\n","        y = p.get_y() + p.get_height()\n","        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n","        \n","\n","\n","\n","fig.savefig(os.path.join(config.train_logs_path, \"result.png\"), format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","fig.savefig(os.path.join(config.train_logs_path, \"result.svg\"), format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## More visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.227262Z","iopub.status.idle":"2023-02-25T14:22:25.228060Z"},"trusted":true},"outputs":[],"source":["def compute_results(model,\n","                    dataloader,\n","                    treshold=0.33):\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    results = {\"Id\": [],\"image\": [], \"GT\": [],\"Prediction\": []}\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader):\n","            id_, imgs, targets = data['Id'], data['image'], data['mask']\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            logits = model(imgs)\n","            probs = torch.sigmoid(logits)\n","            \n","            predictions = (probs >= treshold).float()\n","            predictions =  predictions.cpu()\n","            targets = targets.cpu()\n","            \n","            results[\"Id\"].append(id_)\n","            results[\"image\"].append(imgs.cpu())\n","            results[\"GT\"].append(targets)\n","            results[\"Prediction\"].append(predictions)\n","            \n","            # only 5 pars\n","            if (i > 5):    \n","                return results\n","        return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.229531Z","iopub.status.idle":"2023-02-25T14:22:25.230446Z"},"trusted":true},"outputs":[],"source":["# %%time\n","results = compute_results(\n","    model, test_dataloader, 0.33)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for id_, img, gt, prediction in zip(results['Id'][4:],\n","                                    results['image'][4:],\n","                                    results['GT'][4:],\n","                                    results['Prediction'][4:]\n","                                    ):\n","\n","    print(id_)\n","    name = id_[0]\n","\n","    img = img.squeeze()[0].cpu().detach().numpy()\n","    gt0 = gt[0][0].squeeze().cpu().detach().numpy()\n","    gt1 = gt[0][1].squeeze().cpu().detach().numpy()\n","    gt2 = gt[0][2].squeeze().cpu().detach().numpy()\n","    pred0 = prediction[0][0].squeeze().cpu().detach().numpy()\n","    pred1 = prediction[0][1].squeeze().cpu().detach().numpy()\n","    pred2 = prediction[0][2].squeeze().cpu().detach().numpy()\n","\n","    print(gt.shape)\n","    print(prediction.shape)\n","    print(np.unique(gt))\n","    print(np.unique(prediction))\n","\n","    for i in range(40, 110+1, 5):\n","        fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(\n","            1, 7, figsize=(30, 10))\n","        slice_w = i\n","        ax1.imshow(img[slice_w, :, :], cmap=\"gray\")\n","        ax1.set_title(f'img_{name}_slice_{i}')\n","        ax2.imshow(gt0[slice_w, :, :], cmap=\"bone\")\n","        ax2.set_title(f'WT_GT_{name}_slice_{i}')\n","        ax3.imshow(pred0[slice_w, :, :], cmap=\"bone\")\n","        ax3.set_title(f'WT_Pred_{name}_slice_{i}')\n","        ax4.imshow(gt1[slice_w, :, :], cmap=\"hot\")\n","        ax4.set_title(f'TC_GT_{name}_slice_{i}')\n","        ax5.imshow(pred1[slice_w, :, :], cmap=\"hot\")\n","        ax5.set_title(f'TC_Pred_{name}_slice_{i}')\n","        ax6.imshow(gt2[slice_w, :, :], cmap=\"inferno\")\n","        ax6.set_title(f'ET_GT_{name}_slice_{i}')\n","        ax7.imshow(pred2[slice_w, :, :], cmap=\"inferno\")\n","        ax7.set_title(f'ET_Pred_{name}_slice_{i}')\n","        fig.savefig(os.path.join(config.train_logs_path, f\"prediction_{name}_slice_{i}.png\"),\n","                    format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","    break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchviz import make_dot\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# with torch.no_grad():\n","data = next(iter(test_dataloader))\n","id_, imgs, targets = data['Id'], data['image'], data['mask']\n","imgs, targets = imgs.to(device), targets.to(device)\n","logits = model(imgs)\n","graph = make_dot(logits, params=dict(model.named_parameters()))\n","graph\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph.render(os.path.join(config.train_logs_path, \"model\"), format=\"png\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"be758f4e65d39904e29fa102fec3d5df4c28e93d12e2ff9b541d16e38b6ab85f"}}},"nbformat":4,"nbformat_minor":4}
