{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data downloaded from [here](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation)\n","\n","All BraTS multimodal scans are available as NIfTI files (.nii.gz) and describe a) native (T1) and b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes.\n","\n","Annotations comprise the GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor core (NCR/NET — label 1).\n","\n","# IMPORTANT: Apparently training data 355's segmentation file is named incorrectly in the original dataset; manual renaming to the correct naming convention is required to prevent errors \n","### Alternatively, skip 355 by uncommenting the appropriate line of code under the \"Preprocessing\" section\n","\n","### Code referenced from [this kaggle notebook](https://www.kaggle.com/code/polomarco/brats20-3dunet-3dautoencoder/data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:03.099485Z","iopub.status.busy":"2023-02-25T14:22:03.099128Z","iopub.status.idle":"2023-02-25T14:22:06.945961Z","shell.execute_reply":"2023-02-25T14:22:06.944942Z","shell.execute_reply.started":"2023-02-25T14:22:03.099454Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import time\n","from datetime import datetime\n","from random import randint\n","\n","import numpy as np\n","from scipy import stats\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVR\n","from sklearn.model_selection import KFold\n","\n","import nibabel as nib\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import matplotlib.animation as anim\n","import matplotlib.patches as mpatches\n","import matplotlib.gridspec as gridspec\n","\n","import seaborn as sns\n","from skimage.transform import resize\n","from skimage.util import montage\n","\n","from IPython.display import Image as show_gif\n","from IPython.display import clear_output\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam, AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","import albumentations as A\n","from albumentations import Compose\n","\n","import warnings\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Global config class\n","class GlobalConfig:\n","    # Global config class\n","    root_dir = './BraTS2020'\n","    train_root_dir = './BraTS2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n","    test_root_dir = './BraTS2020/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n","    # CSV where patient ID, path and fold information will be stored\n","    path_to_csv = './fold_data.csv'\n","    # Where your pretrained model is stored (Specifically for the 3DUnet architecture implemented here)\n","    pretrained_model_path = './BraTS2020Logs/model_by_original_notebook_author.pth'\n","    # Where train log is stored; each row (epoch) consists of train/val loss, dice score and jaccard score.  \n","    train_logs_path = './BraTS2020LogsSwinUNETR_BCEDice_Cropped_Fixed_AdamW' #TO DO: Change this for changes in different hyperparam/Model \n","    # for reproducibility\n","    seed = 42\n","\n","\n","def seed_everything(seed: int):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","\n","def load_img(file_path):\n","    data = nib.load(file_path)\n","    data = np.asarray(data.dataobj)\n","    return data\n","\n","config = GlobalConfig()\n","seed_everything(config.seed)\n","if not os.path.isdir(config.train_logs_path):\n","    os.mkdir(config.train_logs_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Visualisations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:06.950703Z","iopub.status.busy":"2023-02-25T14:22:06.950307Z","iopub.status.idle":"2023-02-25T14:22:06.995245Z","shell.execute_reply":"2023-02-25T14:22:06.994095Z","shell.execute_reply.started":"2023-02-25T14:22:06.950669Z"},"trusted":true},"outputs":[],"source":["# Change these to visualise brain MRIs of different patients, and at different slices \n","PATIENT_IDX = 10 # 1 - 369\n","SLICE_IDX = 85 # 0 - 154\n","\n","\n","sample_filename = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_flair.nii\"\n","sample_filename_mask = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_seg.nii\"\n","\n","sample_img = load_img(sample_filename)\n","sample_mask = load_img(sample_filename_mask)\n","\n","sample_filename2 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t1.nii\"\n","sample_img2 = load_img(sample_filename2)\n","\n","sample_filename3 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t2.nii\"\n","sample_img3 = load_img(sample_filename3)\n","\n","sample_filename4 = f\"{config.train_root_dir}/BraTS20_Training_{PATIENT_IDX:03d}/BraTS20_Training_{PATIENT_IDX:03d}_t1ce.nii\"\n","sample_img4 = load_img(sample_filename4)\n","\n","# (240, 240, 155), corresponding to height, width and depth\n","print(\"img shape ->\", sample_img.shape)  \n","print(\"mask shape ->\", sample_mask.shape) \n","# [0,1,2,4], corresponding to the background (BG — label 0), necrotic and non-enhancing tumor core (NCR/NET — label 1), the peritumoral edema (ED — label 2) and GD-enhancing tumor (ET — label 4)\n","print(\"mask unique ->\", np.unique(sample_mask)) \n","\n","# In the BraTS challenge, the segmentation performance is evaluated on three partially overlapping sub-regions of tumors,\n","# namely, whole tumor (WT), tumor core (TC), and enhancing tumor (ET).\n","# The WT is the union of ED, NCR/NET, and ET, while the TC includes NCR/NET and ET.\n","# We hence create 3 sets of mask. This will eventually be stacked upon each other during preprocessing in the Dataset Class.\n","mask_WT = sample_mask.copy()\n","mask_WT[mask_WT == 1] = 1\n","mask_WT[mask_WT == 2] = 1\n","mask_WT[mask_WT == 4] = 1\n","\n","mask_TC = sample_mask.copy()\n","mask_TC[mask_TC == 1] = 1\n","mask_TC[mask_TC == 2] = 0\n","mask_TC[mask_TC == 4] = 1\n","\n","mask_ET = sample_mask.copy()\n","mask_ET[mask_ET == 1] = 0\n","mask_ET[mask_ET == 2] = 0\n","mask_ET[mask_ET == 4] = 1\n","\n","\n","# https://matplotlib.org/3.3.2/gallery/images_contours_and_fields/plot_streamplot.html#sphx-glr-gallery-images-contours-and-fields-plot-streamplot-py\n","# https://stackoverflow.com/questions/25482876/how-to-add-legend-to-imshow-in-matplotlib\n","\n","\n","fig = plt.figure(figsize=(20, 10))\n","\n","gs = gridspec.GridSpec(nrows=2, ncols=4, height_ratios=[1, 1.5])\n","\n","#  Varying density along a streamline\n","ax0 = fig.add_subplot(gs[0, 0])\n","flair = ax0.imshow(sample_img[:, :, SLICE_IDX], cmap='bone') # Show all columns and rows, but a specific slice index\n","ax0.set_title(\"FLAIR\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(flair)\n","\n","#  Varying density along a streamline\n","ax1 = fig.add_subplot(gs[0, 1])\n","t1 = ax1.imshow(sample_img2[:, :, SLICE_IDX], cmap='bone')\n","ax1.set_title(\"T1\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t1)\n","\n","#  Varying density along a streamline\n","ax2 = fig.add_subplot(gs[0, 2])\n","t2 = ax2.imshow(sample_img3[:, :, SLICE_IDX], cmap='bone')\n","ax2.set_title(\"T2\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t2)\n","\n","#  Varying density along a streamline\n","ax3 = fig.add_subplot(gs[0, 3])\n","t1ce = ax3.imshow(sample_img4[:, :, SLICE_IDX], cmap='bone')\n","ax3.set_title(\"T1 contrast\", fontsize=18, weight='bold', y=-0.2)\n","fig.colorbar(t1ce)\n","\n","#  Varying density along a streamline\n","ax4 = fig.add_subplot(gs[1, 1:3])\n","\n","#ax4.imshow(np.ma.masked_where(mask_WT[:,:,SLICE_IDX]== False,  mask_WT[:,:,SLICE_IDX]), cmap='summer', alpha=0.6)\n","l1 = ax4.imshow(mask_WT[:, :, SLICE_IDX], cmap='summer',)\n","l2 = ax4.imshow(np.ma.masked_where(mask_TC[:, :, SLICE_IDX] == False,  mask_TC[:, :, SLICE_IDX]), cmap='rainbow', alpha=0.6)\n","l3 = ax4.imshow(np.ma.masked_where(mask_ET[:, :, SLICE_IDX] == False, mask_ET[:, :, SLICE_IDX]), cmap='winter', alpha=0.6)\n","\n","ax4.set_title(\"\", fontsize=20, weight='bold', y=-0.1)\n","\n","_ = [ax.set_axis_off() for ax in [ax0, ax1, ax2, ax3, ax4]]\n","\n","colors = [im.cmap(im.norm(1)) for im in [l1, l2, l3]]\n","labels = ['Non-Enhancing tumor core',\n","          'Peritumoral Edema ', 'GD-enhancing tumor']\n","patches = [mpatches.Patch(\n","    color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n","# put those patched as legend-handles into the legend\n","plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4, fontsize='xx-large',\n","           title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n","\n","plt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\",\n","             fontsize=20, weight='bold')\n","\n","# fig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2,\n","#             transparent=False, bbox_inches='tight')\n","# fig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2,\n","#             transparent=False, bbox_inches='tight')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Preprocessing\n","\n","### Mapping different patients (and the paths to their data) to different training folds; Fold 0 will be used as validation set; rest will be used as training set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:08.769292Z","iopub.status.busy":"2023-02-25T14:22:08.768899Z","iopub.status.idle":"2023-02-25T14:22:09.238336Z","shell.execute_reply":"2023-02-25T14:22:09.237426Z","shell.execute_reply.started":"2023-02-25T14:22:08.769252Z"},"trusted":true},"outputs":[],"source":["survivalInfoPath = f\"{config.train_root_dir}/survival_info.csv\"\n","nameMappingPath = f\"{config.train_root_dir}/name_mapping.csv\"\n","survival_info_df = pd.read_csv(survivalInfoPath)\n","name_mapping_df = pd.read_csv(nameMappingPath)\n","\n","name_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n","\n","\n","df = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n","df = df[[\"Brats20ID\"]]\n","paths = []\n","for _, row  in df.iterrows():\n","    \n","    id_ = row['Brats20ID']\n","    path = os.path.join(config.train_root_dir, id_)\n","    paths.append(path)\n","    \n","df['path'] = paths\n","\n","train_data = df\n","\n","\"\"\"Uncomment the following if skipping number 355 due to errors\"\"\"\n","# train_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n","\n","kf = KFold(n_splits=7, random_state=config.seed, shuffle=True)\n","for i, (train_index, val_index) in enumerate(kf.split(train_data)):\n","    # assign all rows at val_index to the ith fold\n","    train_data.loc[val_index, \"fold\"] = i\n","\n","\n","train_data.to_csv(config.path_to_csv, index=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Dataset DataLoader\n","\n","### TO-DO: Try different data augmentation techniques"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.261182Z","iopub.status.busy":"2023-02-25T14:22:09.260805Z","iopub.status.idle":"2023-02-25T14:22:09.284501Z","shell.execute_reply":"2023-02-25T14:22:09.283648Z","shell.execute_reply.started":"2023-02-25T14:22:09.261147Z"},"trusted":true},"outputs":[],"source":["class BratsDataset(Dataset):\n","    def __init__(self, df: pd.DataFrame, phase: str=\"test\", do_resizing: bool=False):\n","        # Dataframe containing patient, path and fold mapping information\n","        self.df = df\n","        # \"train\" \"valid\" or \"test\". Determines whether to apply preprocessing\n","        self.phase = phase\n","        self.augmentations = self.get_augmentations(phase)\n","        self.data_types = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\n","        self.do_resizing = do_resizing\n","        \n","    def __len__(self): \n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, idx): # Makes class accessible by square-bracket notations; determines behaviour upon square-bracket access\n","        id_ = self.df.loc[idx, 'Brats20ID']\n","        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n","        images = []\n","        # load all modalities(t1, t1ce, t2, flair)\n","        for data_type in self.data_types:\n","            img_path = os.path.join(root_path, id_ + data_type)\n","            img = self.load_img(img_path)#.transpose(2, 0, 1)\n","            \n","            if self.do_resizing:\n","                img = self.resize(img)\n","    \n","            img = self.normalize(img)\n","            images.append(img)\n","        # stack all 4 modalities into single array as model input\n","        img = np.stack(images)\n","        # move axes of array to new position; \n","        # original shape - (4,240,240,155) -> (4,155,240,240) - new shape; facilitates input into 3DUnet model\n","        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n","        \n","        mask_path =  os.path.join(root_path, id_ + \"_seg.nii\")\n","        mask = self.load_img(mask_path)\n","        \n","        if self.do_resizing:\n","            mask = self.resize(mask)\n","        \n","            \n","        # Creates and stacks appropriate (possibly overlapping) masks as per BraTS challenge. More info in function.\n","        mask = self.preprocess_mask_labels(mask)\n","        \n","        mask = np.clip(mask.astype(np.uint8), 0, 1).astype(np.float32)\n","        mask = np.clip(mask, 0, 1)\n","\n","        # Peform augmentations\n","        augmented = self.augmentations(image=img.astype(np.float32), mask=mask.astype(np.float32))\n","        img = augmented['image']\n","        mask = augmented['mask']\n","\n","        # Returns dictionary with Id, image (x) and mask (y) in train/val phase, else return only id and image (x) in test phase\n","        return {\n","            \"Id\": id_,\n","            \"image\": img,\n","            \"mask\": mask,\n","        }\n","    # TO-DO: Implement possible augmentations here? Lower priority for now\n","    def get_augmentations(self, phase):\n","        list_transforms = []\n","        list_trfms = Compose(list_transforms)\n","        return list_trfms\n","    \n","    def load_img(self, file_path):\n","        data = nib.load(file_path)\n","        data = np.asarray(data.dataobj)\n","        return data\n","    \n","    def normalize(self, data: np.ndarray):\n","        data_min = np.min(data)\n","        return (data - data_min) / (np.max(data) - data_min)\n","    \n","    def resize(self, data: np.ndarray):\n","        # data = resize(data, (224, 224, 128), preserve_range=True)\n","        data = self.crop_3d_array(data, (224, 224, 128))\n","        return data\n","    \n","    def preprocess_mask_labels(self, mask: np.ndarray):\n","        # In the BraTS challenge, the segmentation performance is evaluated on three partially overlapping sub-regions of tumors,\n","        # namely, whole tumor (WT), tumor core (TC), and enhancing tumor (ET).\n","        # The WT is the union of ED, NCR/NET, and ET, while the TC includes NCR/NET and ET.\n","\n","        mask_WT = mask.copy()\n","        mask_WT[mask_WT == 1] = 1\n","        mask_WT[mask_WT == 2] = 1\n","        mask_WT[mask_WT == 4] = 1\n","\n","        mask_TC = mask.copy()\n","        mask_TC[mask_TC == 1] = 1\n","        mask_TC[mask_TC == 2] = 0\n","        mask_TC[mask_TC == 4] = 1\n","\n","        mask_ET = mask.copy()\n","        mask_ET[mask_ET == 1] = 0\n","        mask_ET[mask_ET == 2] = 0\n","        mask_ET[mask_ET == 4] = 1\n","\n","        mask = np.stack([mask_WT, mask_TC, mask_ET])\n","        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n","\n","        return mask\n","\n","    def crop_3d_array(self, arr, crop_shape):\n","        \"\"\"\n","        Crop a 3D array to the specified shape.\n","        \n","        Parameters\n","        ----------\n","        arr : numpy.ndarray\n","            The 3D input array to be cropped.\n","        crop_shape : tuple\n","            The shape of the cropped array. Must be a 3-element tuple (depth, height, width).\n","            \n","        Returns\n","        -------\n","        numpy.ndarray\n","            The cropped array.\n","        \"\"\"\n","\n","        assert len(crop_shape) == 3, \"crop_shape must be a 3-element tuple\"\n","        assert crop_shape[0] <= arr.shape[0], \"depth of crop_shape must be <= depth of arr\"\n","        assert crop_shape[1] <= arr.shape[1], \"height of crop_shape must be <= height of arr\"\n","        assert crop_shape[2] <= arr.shape[2], \"width of crop_shape must be <= width of arr\"\n","\n","        depth_diff = arr.shape[0] - crop_shape[0]\n","        height_diff = arr.shape[1] - crop_shape[1]\n","        width_diff = arr.shape[2] - crop_shape[2]\n","\n","        if depth_diff % 2 == 0:\n","            depth_crop_start = depth_diff // 2\n","            depth_crop_end = arr.shape[0] - (depth_diff // 2)\n","        else:\n","            depth_crop_start = depth_diff // 2\n","            depth_crop_end = arr.shape[0] - (depth_diff // 2) - 1\n","\n","        if height_diff % 2 == 0:\n","            height_crop_start = height_diff // 2\n","            height_crop_end = arr.shape[1] - (height_diff // 2)\n","        else:\n","            height_crop_start = height_diff // 2\n","            height_crop_end = arr.shape[1] - (height_diff // 2) - 1\n","\n","        if width_diff % 2 == 0:\n","            width_crop_start = width_diff // 2\n","            width_crop_end = arr.shape[2] - (width_diff // 2)\n","        else:\n","            width_crop_start = width_diff // 2\n","            width_crop_end = arr.shape[2] - (width_diff // 2) - 1\n","\n","        cropped_arr = arr[depth_crop_start:depth_crop_end,\n","                        height_crop_start:height_crop_end, width_crop_start:width_crop_end]\n","\n","        return cropped_arr\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.286304Z","iopub.status.busy":"2023-02-25T14:22:09.285770Z","iopub.status.idle":"2023-02-25T14:22:09.297705Z","shell.execute_reply":"2023-02-25T14:22:09.296767Z","shell.execute_reply.started":"2023-02-25T14:22:09.286267Z"},"trusted":true},"outputs":[],"source":["def get_dataloaders(\n","    dataset: torch.utils.data.Dataset,\n","    path_to_csv: str,\n","    # phase: str,\n","    val_fold: int = 0, # Choose which fold to be the validation fold\n","    test_fold: int = 1,\n","    batch_size: int = 1,\n","    num_workers: int = 4,\n","    do_resizing: bool = False,\n","):\n","    assert(val_fold != test_fold)\n","    \n","    df = pd.read_csv(path_to_csv)\n","    \n","    '''Returns: dataloader for the model training'''\n","    # Data in folds other than 0 are used for training\n","    train_df = df.loc[~df['fold'].isin([val_fold, test_fold])].reset_index(drop=True)\n","    # Data in fold 0 is used for validation\n","    val_df = df.loc[df['fold'] == val_fold].reset_index(drop=True)\n","    test_df = df.loc[df['fold'] == test_fold].reset_index(drop=True)\n","    \n","    # dataset = dataset(df, phase)\n","    train_dataset = dataset(train_df, \"train\", do_resizing=do_resizing)\n","    val_dataset = dataset(val_df, \"val\", do_resizing=do_resizing)\n","    test_dataset = dataset(test_df, \"test\", do_resizing=do_resizing)\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    test_dataloader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    return train_dataloader, val_dataloader, test_dataloader"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Metric and Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coef_metric(probabilities: torch.Tensor,\n","                     truth: torch.Tensor,\n","                     treshold: float = 0.5,\n","                     eps: float = 1e-9) -> np.ndarray:\n","    \"\"\"\n","    Calculate Dice Score for data batch.\n","    2 * the Area of Overlap divided by the total number of pixels in both images\n","    Also known as F1\n","    Params:\n","        probobilities: model outputs after activation function.\n","        truth: truth values.\n","        threshold: threshold for probabilities.\n","        eps: additive to refine the estimate.\n","        Returns: dice score aka f1.\n","    \"\"\"\n","    scores = []\n","    num = probabilities.shape[0]\n","    predictions = (probabilities >= treshold).float()\n","    assert (predictions.shape == truth.shape)\n","    for i in range(num):\n","        prediction = predictions[i]\n","        truth_ = truth[i]\n","        intersection = 2.0 * (truth_ * prediction).sum()\n","        union = truth_.sum() + prediction.sum()\n","        if truth_.sum() == 0 and prediction.sum() == 0:\n","            scores.append(1.0)\n","        else:\n","            scores.append((intersection + eps) / union)\n","    return np.mean(scores)\n","\n","\n","def jaccard_coef_metric(probabilities: torch.Tensor,\n","                        truth: torch.Tensor,\n","                        treshold: float = 0.5,\n","                        eps: float = 1e-9) -> np.ndarray:\n","    \"\"\"\n","    Calculate Jaccard index for data batch.\n","    Area of Overlap over Area of Union\n","    Also known as Intersection-Over-Union (IoU) \n","    Params:\n","        probobilities: model outputs after activation function.\n","        truth: truth values.\n","        threshold: threshold for probabilities.\n","        eps: additive to refine the estimate.\n","        Returns: jaccard score aka iou.\"\n","    \"\"\"\n","    scores = []\n","    num = probabilities.shape[0]\n","    predictions = (probabilities >= treshold).float()\n","    assert (predictions.shape == truth.shape)\n","\n","    for i in range(num):\n","        prediction = predictions[i]\n","        truth_ = truth[i]\n","        intersection = (prediction * truth_).sum()\n","        union = (prediction.sum() + truth_.sum()) - intersection + eps\n","        if truth_.sum() == 0 and prediction.sum() == 0:\n","            scores.append(1.0)\n","        else:\n","            scores.append((intersection + eps) / union)\n","    return np.mean(scores)\n","\n","# Used to accumulate and calculate metrices per epoch\n","\n","\n","class Meter:\n","    '''factory for storing and updating iou and dice scores.'''\n","\n","    def __init__(self, treshold: float = 0.5):\n","        self.threshold: float = treshold\n","        self.dice_scores: list = []\n","        self.iou_scores: list = []\n","\n","    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n","        \"\"\"\n","        Takes: logits from output model and targets,\n","        calculates dice and iou scores, and stores them in lists.\n","        \"\"\"\n","        probs = torch.sigmoid(logits)\n","        dice = dice_coef_metric(probs, targets, self.threshold)\n","        iou = jaccard_coef_metric(probs, targets, self.threshold)\n","\n","        self.dice_scores.append(dice)\n","        self.iou_scores.append(iou)\n","\n","    def get_metrics(self) -> np.ndarray:\n","        \"\"\"\n","        Returns: the average of the accumulated dice and iou scores.\n","        \"\"\"\n","        dice = np.mean(self.dice_scores)\n","        iou = np.mean(self.iou_scores)\n","        return dice, iou\n","\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"Calculate dice loss.\"\"\"\n","\n","    def __init__(self, eps: float = 1e-9):\n","        super(DiceLoss, self).__init__()\n","        self.eps = eps\n","\n","    def forward(self,\n","                logits: torch.Tensor,\n","                targets: torch.Tensor) -> torch.Tensor:\n","\n","        num = targets.size(0)\n","        probability = torch.sigmoid(logits)\n","        probability = probability.view(num, -1)\n","        targets = targets.view(num, -1)\n","        assert (probability.shape == targets.shape)\n","\n","        intersection = 2.0 * (probability * targets).sum()\n","        union = probability.sum() + targets.sum()\n","        dice_score = (intersection + self.eps) / union\n","        #print(\"intersection\", intersection, union, dice_score)\n","        return 1.0 - dice_score\n","\n","\n","class BCEDiceLoss(nn.Module):\n","    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n","\n","    def __init__(self):\n","        super(BCEDiceLoss, self).__init__()\n","        self.bce = nn.BCEWithLogitsLoss()\n","        self.dice = DiceLoss()\n","\n","    def forward(self,\n","                logits: torch.Tensor,\n","                targets: torch.Tensor) -> torch.Tensor:\n","        assert (logits.shape == targets.shape)\n","        dice_loss = self.dice(logits, targets)\n","        bce_loss = self.bce(logits, targets)\n","\n","        return bce_loss + dice_loss\n","\n","# helper functions for testing.\n","\n","\n","def dice_coef_metric_per_classes(probabilities: np.ndarray,\n","                                 truth: np.ndarray,\n","                                 treshold: float = 0.5,\n","                                 eps: float = 1e-9,\n","                                 classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n","    \"\"\"\n","    Calculate Dice score for data batch and for each class.\n","    Params:\n","        probobilities: model outputs after activation function.\n","        truth: model targets.\n","        threshold: threshold for probabilities.\n","        eps: additive to refine the estimate.\n","        classes: list with name classes.\n","        Returns: dict with dice scores for each class.\n","    \"\"\"\n","    scores = {key: list() for key in classes}\n","    num = probabilities.shape[0]\n","    num_classes = probabilities.shape[1]\n","    predictions = (probabilities >= treshold).astype(np.float32)\n","    assert (predictions.shape == truth.shape)\n","\n","    for i in range(num):\n","        for class_ in range(num_classes):\n","            prediction = predictions[i][class_]\n","            truth_ = truth[i][class_]\n","            intersection = 2.0 * (truth_ * prediction).sum()\n","            union = truth_.sum() + prediction.sum()\n","            if truth_.sum() == 0 and prediction.sum() == 0:\n","                scores[classes[class_]].append(1.0)\n","            else:\n","                scores[classes[class_]].append((intersection + eps) / union)\n","\n","    return scores\n","\n","\n","def jaccard_coef_metric_per_classes(probabilities: np.ndarray,\n","                                    truth: np.ndarray,\n","                                    treshold: float = 0.5,\n","                                    eps: float = 1e-9,\n","                                    classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n","    \"\"\"\n","    Calculate Jaccard index for data batch and for each class.\n","    Params:\n","        probobilities: model outputs after activation function.\n","        truth: model targets.\n","        threshold: threshold for probabilities.\n","        eps: additive to refine the estimate.\n","        classes: list with name classes.\n","        Returns: dict with jaccard scores for each class.\"\n","    \"\"\"\n","    scores = {key: list() for key in classes}\n","    num = probabilities.shape[0]\n","    num_classes = probabilities.shape[1]\n","    predictions = (probabilities >= treshold).astype(np.float32)\n","    assert (predictions.shape == truth.shape)\n","\n","    for i in range(num):\n","        for class_ in range(num_classes):\n","            prediction = predictions[i][class_]\n","            truth_ = truth[i][class_]\n","            intersection = (prediction * truth_).sum()\n","            union = (prediction.sum() + truth_.sum()) - intersection + eps\n","            if truth_.sum() == 0 and prediction.sum() == 0:\n","                scores[classes[class_]].append(1.0)\n","            else:\n","                scores[classes[class_]].append((intersection + eps) / union)\n","\n","    return scores\n"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:24.970570Z","iopub.status.busy":"2023-02-25T14:22:24.970150Z","iopub.status.idle":"2023-02-25T14:22:25.005465Z","shell.execute_reply":"2023-02-25T14:22:25.004123Z","shell.execute_reply.started":"2023-02-25T14:22:24.970535Z"},"trusted":true},"outputs":[],"source":["\n","\n","class Trainer:\n","    \"\"\"\n","    Factory for training proccess.\n","    Args:\n","        display_plot: if True - plot train history after each epoch.\n","        net: neural network for mask prediction.\n","        criterion: factory for calculating objective loss.\n","        optimizer: optimizer for weights updating.\n","        phases: list with train and validation phases.\n","        dataloaders: dict with data loaders for train and val phases.\n","        path_to_csv: path to csv file.\n","        meter: factory for storing and updating metrics.\n","        batch_size: data batch size for one step weights updating.\n","        num_epochs: num weights updation for all data.\n","        accumulation_steps: the number of steps after which the optimization step can be taken\n","                    (https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614).\n","        lr: learning rate for optimizer.\n","        scheduler: scheduler for control learning rate.\n","        losses: dict for storing lists with losses for each phase.\n","        jaccard_scores: dict for storing lists with jaccard scores for each phase.\n","        dice_scores: dict for storing lists with dice scores for each phase.\n","    \"\"\"\n","    def __init__(self,\n","                 net: nn.Module,\n","                 dataset: torch.utils.data.Dataset,\n","                 criterion: nn.Module,\n","                 lr: float,\n","                 accumulation_steps: int,\n","                 batch_size: int,\n","                 val_fold: int,\n","                 test_fold: int,\n","                 num_epochs: int,\n","                 path_to_csv: str,\n","                 display_plot: bool = True,\n","                 do_resizing: bool = False,\n","                 optimizer: torch.optim = Adam\n","                ):\n","\n","        \"\"\"Initialization.\"\"\"\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        print(\"device:\", self.device)\n","        self.display_plot = display_plot\n","        self.net = net\n","        self.net = self.net.to(self.device)\n","        self.criterion = criterion\n","        self.optimizer = optimizer(self.net.parameters(), lr=lr)\n","        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\",  # Reduces learning rate when a metric has stopped improving\n","                                           patience=2, verbose=True)\n","        self.accumulation_steps = accumulation_steps // batch_size\n","        self.phases = [\"train\", \"val\"]\n","        self.num_epochs = num_epochs\n","        train_dl, val_dl, test_dl = get_dataloaders(\n","            dataset=dataset,\n","            path_to_csv=path_to_csv,\n","            val_fold=val_fold,\n","            test_fold=test_fold,\n","            batch_size=batch_size,\n","            num_workers=4,\n","            do_resizing=do_resizing,\n","        )\n","        self.dataloaders = {\n","            \"train\": train_dl,\n","            \"val\": val_dl,\n","            \"test\": test_dl\n","        }\n","        self.best_loss = float(\"inf\")\n","        self.losses = {phase: [] for phase in self.phases}\n","        self.dice_scores = {phase: [] for phase in self.phases}\n","        self.jaccard_scores = {phase: [] for phase in self.phases}\n","        self.last_completed_run_time = None\n","         \n","    def _compute_loss_and_outputs(self,\n","                                  images: torch.Tensor,\n","                                  targets: torch.Tensor):\n","        images = images.to(self.device)\n","        targets = targets.to(self.device)\n","        logits = self.net(images)\n","        loss = self.criterion(logits, targets)\n","        return loss, logits\n","        \n","    def _do_epoch(self, epoch: int, phase: str):\n","        # print(f\"{phase} epoch: {epoch + 1} | time: {time.strftime('%H:%M:%S')}\")\n","\n","        self.net.train() if phase == \"train\" else self.net.eval()\n","        meter = Meter()\n","        dataloader = self.dataloaders[phase]\n","        total_batches = len(dataloader)\n","        running_loss = 0.0\n","        self.optimizer.zero_grad()\n","        t_dataloader = tqdm(enumerate(dataloader), unit=\"batch\", total=total_batches)\n","        for itr, data_batch in t_dataloader:\n","            t_dataloader.set_description(f\"{phase} epoch: {epoch + 1} | time: {time.strftime('%H:%M:%S')}\")\n","            images, targets = data_batch['image'], data_batch['mask']\n","            loss, logits = self._compute_loss_and_outputs(images, targets)\n","            loss = loss / self.accumulation_steps\n","            t_dataloader.set_postfix(loss=loss.item())\n","            if phase == \"train\":\n","                loss.backward()\n","                if (itr + 1) % self.accumulation_steps == 0:\n","                    self.optimizer.step()\n","                    self.optimizer.zero_grad()\n","            running_loss += loss.item()\n","            meter.update(logits.detach().cpu(),\n","                         targets.detach().cpu()\n","                        )\n","            \n","        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n","        epoch_dice, epoch_iou = meter.get_metrics()\n","        \n","        self.losses[phase].append(epoch_loss)\n","        self.dice_scores[phase].append(epoch_dice)\n","        self.jaccard_scores[phase].append(epoch_iou)\n","\n","        return epoch_loss\n","        \n","    def run(self):\n","        start = datetime.now()\n","        for epoch in range(self.num_epochs):\n","            self._do_epoch(epoch, \"train\")\n","            with torch.no_grad():\n","                val_loss = self._do_epoch(epoch, \"val\")\n","                self.scheduler.step(val_loss)\n","            if self.display_plot:\n","                self._plot_train_history()\n","                \n","            if val_loss < self.best_loss:\n","                print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n","                self.best_loss = val_loss\n","                now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","                checkpoint_filename = f\"your_best_model_{now}.pth\"\n","                torch.save(self.net.state_dict(), os.path.join(\n","                    config.train_logs_path, checkpoint_filename))\n","            print()\n","        self.last_completed_run_time = str(datetime.now() - start)\n","        self._save_train_history()\n","            \n","    def _plot_train_history(self):\n","        data = [self.losses, self.dice_scores, self.jaccard_scores]\n","        colors = ['deepskyblue', \"crimson\"]\n","        labels = [\n","            f\"\"\"\n","            train loss {self.losses['train'][-1]}\n","            val loss {self.losses['val'][-1]}\n","            \"\"\",\n","            \n","            f\"\"\"\n","            train dice score {self.dice_scores['train'][-1]}\n","            val dice score {self.dice_scores['val'][-1]} \n","            \"\"\", \n","                  \n","            f\"\"\"\n","            train jaccard score {self.jaccard_scores['train'][-1]}\n","            val jaccard score {self.jaccard_scores['val'][-1]}\n","            \"\"\",\n","        ]\n","        \n","        clear_output(True)\n","        with plt.style.context(\"seaborn-dark-palette\"):\n","            fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n","            for i, ax in enumerate(axes):\n","                ax.plot(data[i]['val'], c=colors[0], label=\"val\")\n","                ax.plot(data[i]['train'], c=colors[-1], label=\"train\")\n","                ax.set_title(labels[i])\n","                ax.legend(loc=\"upper right\")\n","                \n","            plt.tight_layout()\n","            plt.show()\n","            \n","    def load_predtrain_model(self,\n","                             state_path: str):\n","        self.net.load_state_dict(torch.load(state_path))\n","        print(\"Predtrain model loaded\")\n","        \n","    def _save_train_history(self):\n","        \"\"\"writing model weights and training logs to files.\"\"\"\n","        now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","        checkpoint_filename = f\"your_last_epoch_model_{now}.pth\"\n","        torch.save(self.net.state_dict(),os.path.join(\n","            config.train_logs_path, checkpoint_filename))\n","\n","        logs_ = [self.losses, self.dice_scores, self.jaccard_scores]\n","        log_names_ = [\"_loss\", \"_dice\", \"_jaccard\"]\n","        logs = [logs_[i][key] for i in list(range(len(logs_)))\n","                         for key in logs_[i]]\n","        log_names = [key+log_names_[i] \n","                     for i in list(range(len(logs_))) \n","                     for key in logs_[i]\n","                    ]\n","        # train_logs_path = './BraTS2020Logs/train_log.csv'\n","        pd.DataFrame(\n","            dict(zip(log_names, logs))\n","        ).to_csv(os.path.join(config.train_logs_path, f\"train_log_{now}.csv\"), index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","import time\n","start = datetime.now()\n","time.sleep(5.3)\n","print()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3DUnet\n","\n","### TO-DO: Try different model architectures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, num_groups=8):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels,\n","                      kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm3d(out_channels),\n","            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv3d(out_channels, out_channels,\n","                      kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm3d(out_channels),\n","            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.MaxPool3d(2, 2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.encoder(x)\n","\n","\n","class Up(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, trilinear=True):\n","        super().__init__()\n","\n","        if trilinear:\n","            self.up = nn.Upsample(\n","                scale_factor=2, mode='trilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose3d(\n","                in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n","\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        diffZ = x2.size()[2] - x1.size()[2]\n","        diffY = x2.size()[3] - x1.size()[3]\n","        diffX = x2.size()[4] - x1.size()[4]\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY //\n","                   2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class Out(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class UNet3d(nn.Module):\n","    def __init__(self, in_channels, n_classes, n_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.n_classes = n_classes\n","        self.n_channels = n_channels\n","\n","        self.conv = DoubleConv(in_channels, n_channels)\n","        self.enc1 = Down(n_channels, 2 * n_channels)\n","        self.enc2 = Down(2 * n_channels, 4 * n_channels)\n","        self.enc3 = Down(4 * n_channels, 8 * n_channels)\n","        self.enc4 = Down(8 * n_channels, 8 * n_channels)\n","\n","        self.dec1 = Up(16 * n_channels, 4 * n_channels)\n","        self.dec2 = Up(8 * n_channels, 2 * n_channels)\n","        self.dec3 = Up(4 * n_channels, n_channels)\n","        self.dec4 = Up(2 * n_channels, n_channels)\n","        self.out = Out(n_channels, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.conv(x)\n","        x2 = self.enc1(x1)\n","        x3 = self.enc2(x2)\n","        x4 = self.enc3(x3)\n","        x5 = self.enc4(x4)\n","\n","        mask = self.dec1(x5, x4)\n","        mask = self.dec2(mask, x3)\n","        mask = self.dec3(mask, x2)\n","        mask = self.dec4(mask, x1)\n","        mask = self.out(mask)\n","        return mask\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3D UNet with Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DoubleConvDropout(nn.Module):\n","    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, num_groups=8):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels,\n","                      kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm3d(out_channels),\n","            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n","            nn.Dropout(p=0.2),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv3d(out_channels, out_channels,\n","                      kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm3d(out_channels),\n","            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n","            nn.Dropout(p=0.2),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class DownDropout(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.MaxPool3d(2, 2),\n","            DoubleConvDropout(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.encoder(x)\n","\n","\n","class UpDropout(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, trilinear=True):\n","        super().__init__()\n","\n","        if trilinear:\n","            self.up = nn.Upsample(\n","                scale_factor=2, mode='trilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose3d(\n","                in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n","\n","        self.conv = DoubleConvDropout(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        diffZ = x2.size()[2] - x1.size()[2]\n","        diffY = x2.size()[3] - x1.size()[3]\n","        diffX = x2.size()[4] - x1.size()[4]\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY //\n","                   2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class Out(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class UNet3dDropout(nn.Module):\n","    def __init__(self, in_channels, n_classes, n_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.n_classes = n_classes\n","        self.n_channels = n_channels\n","\n","        self.conv = DoubleConvDropout(in_channels, n_channels)\n","        self.enc1 = DownDropout(n_channels, 2 * n_channels)\n","        self.enc2 = DownDropout(2 * n_channels, 4 * n_channels)\n","        self.enc3 = DownDropout(4 * n_channels, 8 * n_channels)\n","        self.enc4 = DownDropout(8 * n_channels, 8 * n_channels)\n","\n","        self.dec1 = UpDropout(16 * n_channels, 4 * n_channels)\n","        self.dec2 = UpDropout(8 * n_channels, 2 * n_channels)\n","        self.dec3 = UpDropout(4 * n_channels, n_channels)\n","        self.dec4 = UpDropout(2 * n_channels, n_channels)\n","        self.out = Out(n_channels, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.conv(x)\n","        x2 = self.enc1(x1)\n","        x3 = self.enc2(x2)\n","        x4 = self.enc3(x3)\n","        x5 = self.enc4(x4)\n","\n","        mask = self.dec1(x5, x4)\n","        mask = self.dec2(mask, x3)\n","        mask = self.dec3(mask, x2)\n","        mask = self.dec4(mask, x1)\n","        mask = self.out(mask)\n","        return mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Copyright (c) MONAI Consortium\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","import itertools\n","from typing import Optional, Sequence, Tuple, Type, Union\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from torch.nn import LayerNorm\n","\n","from monai.networks.blocks import MLPBlock as Mlp\n","from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n","from monai.networks.layers import DropPath, trunc_normal_\n","from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n","\n","rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n","\n","__all__ = [\n","    \"SwinUNETR\",\n","    \"window_partition\",\n","    \"window_reverse\",\n","    \"WindowAttention\",\n","    \"SwinTransformerBlock\",\n","    \"PatchMerging\",\n","    \"PatchMergingV2\",\n","    \"MERGING_MODE\",\n","    \"BasicLayer\",\n","    \"SwinTransformer\",\n","]\n","\n","\n","class SwinUNETR(nn.Module):\n","    \"\"\"\n","    Swin UNETR based on: \"Hatamizadeh et al.,\n","    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n","    <https://arxiv.org/abs/2201.01266>\"\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        img_size: Union[Sequence[int], int],\n","        in_channels: int,\n","        out_channels: int,\n","        depths: Sequence[int] = (2, 2, 2, 2),\n","        num_heads: Sequence[int] = (3, 6, 12, 24),\n","        feature_size: int = 24,\n","        norm_name: Union[Tuple, str] = \"instance\",\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        dropout_path_rate: float = 0.0,\n","        normalize: bool = True,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            img_size: dimension of input image.\n","            in_channels: dimension of input channels.\n","            out_channels: dimension of output channels.\n","            feature_size: dimension of network feature size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            norm_name: feature normalization type and arguments.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            dropout_path_rate: drop path rate.\n","            normalize: normalize output intermediate features in each stage.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: number of spatial dims.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","\n","        Examples::\n","\n","            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n","            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n","\n","            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n","            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n","\n","            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n","            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n","\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        img_size = ensure_tuple_rep(img_size, spatial_dims)\n","        patch_size = ensure_tuple_rep(2, spatial_dims)\n","        window_size = ensure_tuple_rep(7, spatial_dims)\n","\n","        if spatial_dims not in (2, 3):\n","            raise ValueError(\"spatial dimension should be 2 or 3.\")\n","\n","        for m, p in zip(img_size, patch_size):\n","            for i in range(5):\n","                if m % np.power(p, i + 1) != 0:\n","                    raise ValueError(\n","                        \"input image size (img_size) should be divisible by stage-wise image resolution.\")\n","\n","        if not (0 <= drop_rate <= 1):\n","            raise ValueError(\"dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= attn_drop_rate <= 1):\n","            raise ValueError(\n","                \"attention dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= dropout_path_rate <= 1):\n","            raise ValueError(\"drop path rate should be between 0 and 1.\")\n","\n","        if feature_size % 12 != 0:\n","            raise ValueError(\"feature_size should be divisible by 12.\")\n","\n","        self.normalize = normalize\n","\n","        self.swinViT = SwinTransformer(\n","            in_chans=in_channels,\n","            embed_dim=feature_size,\n","            window_size=window_size,\n","            patch_size=patch_size,\n","            depths=depths,\n","            num_heads=num_heads,\n","            mlp_ratio=4.0,\n","            qkv_bias=True,\n","            drop_rate=drop_rate,\n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=dropout_path_rate,\n","            norm_layer=nn.LayerNorm,\n","            use_checkpoint=use_checkpoint,\n","            spatial_dims=spatial_dims,\n","            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(\n","                downsample, str) else downsample,\n","        )\n","\n","        self.encoder1 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=in_channels,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder2 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder3 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=2 * feature_size,\n","            out_channels=2 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder4 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=4 * feature_size,\n","            out_channels=4 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder10 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=16 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder5 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=8 * feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder4 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 8,\n","            out_channels=feature_size * 4,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder3 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 4,\n","            out_channels=feature_size * 2,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","        self.decoder2 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 2,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder1 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.out = UnetOutBlock(\n","            spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n","\n","\n","    def load_from(self, weights):\n","\n","        with torch.no_grad():\n","            self.swinViT.patch_embed.proj.weight.copy_(\n","                weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n","            self.swinViT.patch_embed.proj.bias.copy_(\n","                weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n","            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers1\")\n","            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers2\")\n","            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers3\")\n","            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers4\")\n","            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n","            )\n","\n","\n","    def forward(self, x_in):\n","        hidden_states_out = self.swinViT(x_in, self.normalize)\n","        enc0 = self.encoder1(x_in)\n","        enc1 = self.encoder2(hidden_states_out[0])\n","        enc2 = self.encoder3(hidden_states_out[1])\n","        enc3 = self.encoder4(hidden_states_out[2])\n","        dec4 = self.encoder10(hidden_states_out[4])\n","        dec3 = self.decoder5(dec4, hidden_states_out[3])\n","        dec2 = self.decoder4(dec3, enc3)\n","        dec1 = self.decoder3(dec2, enc2)\n","        dec0 = self.decoder2(dec1, enc1)\n","        out = self.decoder1(dec0, enc0)\n","        logits = self.out(out)\n","        return logits\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"window partition operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","    Args:\n","        x: input tensor.\n","        window_size: local window size.\n","    \"\"\"\n","    x_shape = x.size()\n","    if len(x_shape) == 5:\n","        b, d, h, w, c = x_shape\n","        x = x.view(\n","            b,\n","            d // window_size[0],\n","            window_size[0],\n","            h // window_size[1],\n","            window_size[1],\n","            w // window_size[2],\n","            window_size[2],\n","            c,\n","        )\n","        windows = (\n","            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1,\n","                                                                window_size[0] * window_size[1] * window_size[2], c)\n","        )\n","    elif len(x_shape) == 4:\n","        b, h, w, c = x.shape\n","        x = x.view(b, h // window_size[0], window_size[0],\n","                w // window_size[1], window_size[1], c)\n","        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous(\n","        ).view(-1, window_size[0] * window_size[1], c)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, dims):\n","    \"\"\"window reverse operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","    Args:\n","        windows: windows tensor.\n","        window_size: local window size.\n","        dims: dimension values.\n","    \"\"\"\n","    if len(dims) == 4:\n","        b, d, h, w = dims\n","        x = windows.view(\n","            b,\n","            d // window_size[0],\n","            h // window_size[1],\n","            w // window_size[2],\n","            window_size[0],\n","            window_size[1],\n","            window_size[2],\n","            -1,\n","        )\n","        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n","\n","    elif len(dims) == 3:\n","        b, h, w = dims\n","        x = windows.view(\n","            b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n","        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n","    return x\n","\n","\n","def get_window_size(x_size, window_size, shift_size=None):\n","    \"\"\"Computing window size based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","    Args:\n","        x_size: input size.\n","        window_size: local window size.\n","        shift_size: window shifting size.\n","    \"\"\"\n","\n","    use_window_size = list(window_size)\n","    if shift_size is not None:\n","        use_shift_size = list(shift_size)\n","    for i in range(len(x_size)):\n","        if x_size[i] <= window_size[i]:\n","            use_window_size[i] = x_size[i]\n","            if shift_size is not None:\n","                use_shift_size[i] = 0\n","\n","    if shift_size is None:\n","        return tuple(use_window_size)\n","    else:\n","        return tuple(use_window_size), tuple(use_shift_size)\n","\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        qkv_bias: bool = False,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            attn_drop: attention dropout rate.\n","            proj_drop: dropout rate of output.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim**-0.5\n","        mesh_args = torch.meshgrid.__kwdefaults__\n","\n","        if len(self.window_size) == 3:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(\n","                    (2 * self.window_size[0] - 1) * (2 *\n","                                                     self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n","                    num_heads,\n","                )\n","            )\n","            coords_d = torch.arange(self.window_size[0])\n","            coords_h = torch.arange(self.window_size[1])\n","            coords_w = torch.arange(self.window_size[2])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(\n","                    coords_d, coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(\n","                    coords_d, coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :,\n","                                             None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 2] += self.window_size[2] - 1\n","            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * \\\n","                (2 * self.window_size[2] - 1)\n","            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n","        elif len(self.window_size) == 2:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros((2 * window_size[0] - 1)\n","                            * (2 * window_size[1] - 1), num_heads)\n","            )\n","            coords_h = torch.arange(self.window_size[0])\n","            coords_w = torch.arange(self.window_size[1])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(\n","                    coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :,\n","                                             None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\",\n","                             relative_position_index)\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        b, n, c = x.shape\n","        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c //\n","                                  self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.clone(\n","            )[:n, :n].reshape(-1)  # type: ignore\n","        ].reshape(n, n, -1)\n","        relative_position_bias = relative_position_bias.permute(\n","            2, 0, 1).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","        if mask is not None:\n","            nw = mask.shape[0]\n","            attn = attn.view(b // nw, nw, self.num_heads, n,\n","                             n) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, n, n)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn).to(v.dtype)\n","        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer block based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        shift_size: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        act_layer: str = \"GELU\",\n","        norm_layer: Type[LayerNorm] = nn.LayerNorm,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            shift_size: window shift size.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            drop_path: stochastic depth rate.\n","            act_layer: activation layer.\n","            norm_layer: normalization layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        self.use_checkpoint = use_checkpoint\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=self.window_size,\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(\n","            drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim,\n","                       act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    def forward_part1(self, x, mask_matrix):\n","        x_shape = x.size()\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            window_size, shift_size = get_window_size(\n","                (d, h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x.shape\n","            window_size, shift_size = get_window_size(\n","                (h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = 0\n","            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n","            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","            _, hp, wp, _ = x.shape\n","            dims = [b, hp, wp]\n","\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                shifted_x = torch.roll(\n","                    x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                shifted_x = torch.roll(\n","                    x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","        x_windows = window_partition(shifted_x, window_size)\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n","        shifted_x = window_reverse(attn_windows, window_size, dims)\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                x = torch.roll(shifted_x, shifts=(\n","                    shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                x = torch.roll(shifted_x, shifts=(\n","                    shift_size[0], shift_size[1]), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if len(x_shape) == 5:\n","            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n","                x = x[:, :d, :h, :w, :].contiguous()\n","        elif len(x_shape) == 4:\n","            if pad_r > 0 or pad_b > 0:\n","                x = x[:, :h, :w, :].contiguous()\n","\n","        return x\n","\n","    def forward_part2(self, x):\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","    def load_from(self, weights, n_block, layer):\n","        root = f\"module.{layer}.0.blocks.{n_block}.\"\n","        block_names = [\n","            \"norm1.weight\",\n","            \"norm1.bias\",\n","            \"attn.relative_position_bias_table\",\n","            \"attn.relative_position_index\",\n","            \"attn.qkv.weight\",\n","            \"attn.qkv.bias\",\n","            \"attn.proj.weight\",\n","            \"attn.proj.bias\",\n","            \"norm2.weight\",\n","            \"norm2.bias\",\n","            \"mlp.fc1.weight\",\n","            \"mlp.fc1.bias\",\n","            \"mlp.fc2.weight\",\n","            \"mlp.fc2.bias\",\n","        ]\n","        with torch.no_grad():\n","            self.norm1.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[0]])\n","            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n","            self.attn.relative_position_bias_table.copy_(\n","                weights[\"state_dict\"][root + block_names[2]])\n","            self.attn.relative_position_index.copy_(\n","                weights[\"state_dict\"][root + block_names[3]])  # type: ignore\n","            self.attn.qkv.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[4]])\n","            self.attn.qkv.bias.copy_(\n","                weights[\"state_dict\"][root + block_names[5]])\n","            self.attn.proj.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[6]])\n","            self.attn.proj.bias.copy_(\n","                weights[\"state_dict\"][root + block_names[7]])\n","            self.norm2.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[8]])\n","            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n","            self.mlp.linear1.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[10]])\n","            self.mlp.linear1.bias.copy_(\n","                weights[\"state_dict\"][root + block_names[11]])\n","            self.mlp.linear2.weight.copy_(\n","                weights[\"state_dict\"][root + block_names[12]])\n","            self.mlp.linear2.bias.copy_(\n","                weights[\"state_dict\"][root + block_names[13]])\n","\n","    def forward(self, x, mask_matrix):\n","        shortcut = x\n","        if self.use_checkpoint:\n","            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)\n","        else:\n","            x = self.forward_part1(x, mask_matrix)\n","        x = shortcut + self.drop_path(x)\n","        if self.use_checkpoint:\n","            x = x + checkpoint.checkpoint(self.forward_part2, x)\n","        else:\n","            x = x + self.forward_part2(x)\n","        return x\n","\n","\n","class PatchMergingV2(nn.Module):\n","    \"\"\"\n","    Patch merging layer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(self, dim: int, norm_layer: Type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            norm_layer: normalization layer.\n","            spatial_dims: number of spatial dims.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        if spatial_dims == 3:\n","            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(8 * dim)\n","        elif spatial_dims == 2:\n","            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","            x = torch.cat(\n","                [x[:, i::2, j::2, k::2, :]\n","                    for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n","            )\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n","            x = torch.cat([x[:, j::2, i::2, :]\n","                          for i, j in itertools.product(range(2), range(2))], -1)\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","class PatchMerging(PatchMergingV2):\n","    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 4:\n","            return super().forward(x)\n","        if len(x_shape) != 5:\n","            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n","        b, d, h, w, c = x_shape\n","        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","        x0 = x[:, 0::2, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, 0::2, :]\n","        x3 = x[:, 0::2, 0::2, 1::2, :]\n","        x4 = x[:, 1::2, 0::2, 1::2, :]\n","        x5 = x[:, 0::2, 1::2, 0::2, :]\n","        x6 = x[:, 0::2, 0::2, 1::2, :]\n","        x7 = x[:, 1::2, 1::2, 1::2, :]\n","        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n","\n","\n","def compute_mask(dims, window_size, shift_size, device):\n","    \"\"\"Computing region masks based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        dims: dimension values.\n","        window_size: local window size.\n","        shift_size: shift size.\n","        device: device.\n","    \"\"\"\n","\n","    cnt = 0\n","\n","    if len(dims) == 3:\n","        d, h, w = dims\n","        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n","        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n","                    img_mask[:, d, h, w, :] = cnt\n","                    cnt += 1\n","\n","    elif len(dims) == 2:\n","        h, w = dims\n","        img_mask = torch.zeros((1, h, w, 1), device=device)\n","        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","    mask_windows = window_partition(img_mask, window_size)\n","    mask_windows = mask_windows.squeeze(-1)\n","    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","    attn_mask = attn_mask.masked_fill(\n","        attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","    return attn_mask\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        depth: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        drop_path: list,\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = False,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        norm_layer: Type[LayerNorm] = nn.LayerNorm,\n","        downsample: Optional[nn.Module] = None,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            depth: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            drop_path: stochastic depth rate.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            norm_layer: normalization layer.\n","            downsample: an optional downsampling layer at the end of the layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.window_size = window_size\n","        self.shift_size = tuple(i // 2 for i in window_size)\n","        self.no_shift = tuple(0 for i in window_size)\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=self.window_size,\n","                    shift_size=self.no_shift if (\n","                        i % 2 == 0) else self.shift_size,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(\n","                        drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                    use_checkpoint=use_checkpoint,\n","                )\n","                for i in range(depth)\n","            ]\n","        )\n","        self.downsample = downsample\n","        if callable(self.downsample):\n","            self.downsample = downsample(\n","                dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, c, d, h, w = x_shape\n","            window_size, shift_size = get_window_size(\n","                (d, h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c d h w -> b d h w c\")\n","            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n","            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n","            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n","            attn_mask = compute_mask(\n","                [dp, hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, d, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b d h w c -> b c d h w\")\n","\n","        elif len(x_shape) == 4:\n","            b, c, h, w = x_shape\n","            window_size, shift_size = get_window_size(\n","                (h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c h w -> b h w c\")\n","            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n","            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n","            attn_mask = compute_mask(\n","                [hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b h w c -> b c h w\")\n","        return x\n","\n","\n","class SwinTransformer(nn.Module):\n","    \"\"\"\n","    Swin Transformer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans: int,\n","        embed_dim: int,\n","        window_size: Sequence[int],\n","        patch_size: Sequence[int],\n","        depths: Sequence[int],\n","        num_heads: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        drop_path_rate: float = 0.0,\n","        norm_layer: Type[LayerNorm] = nn.LayerNorm,\n","        patch_norm: bool = False,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            in_chans: dimension of input channels.\n","            embed_dim: number of linear projection output channels.\n","            window_size: local window size.\n","            patch_size: patch size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            drop_path_rate: stochastic depth rate.\n","            norm_layer: normalization layer.\n","            patch_norm: add normalization after patch embedding.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: spatial dimension.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","        \"\"\"\n","\n","        super().__init__()\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        self.window_size = window_size\n","        self.patch_size = patch_size\n","        self.patch_embed = PatchEmbed(\n","            patch_size=self.patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n","            spatial_dims=spatial_dims,\n","        )\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(\n","            0, drop_path_rate, sum(depths))]\n","        self.layers1 = nn.ModuleList()\n","        self.layers2 = nn.ModuleList()\n","        self.layers3 = nn.ModuleList()\n","        self.layers4 = nn.ModuleList()\n","        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(\n","            downsample, str) else downsample\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2**i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=self.window_size,\n","                drop_path=dpr[sum(depths[:i_layer])                              : sum(depths[: i_layer + 1])],\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                norm_layer=norm_layer,\n","                downsample=down_sample_mod,\n","                use_checkpoint=use_checkpoint,\n","            )\n","            if i_layer == 0:\n","                self.layers1.append(layer)\n","            elif i_layer == 1:\n","                self.layers2.append(layer)\n","            elif i_layer == 2:\n","                self.layers3.append(layer)\n","            elif i_layer == 3:\n","                self.layers4.append(layer)\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","\n","    def proj_out(self, x, normalize=False):\n","        if normalize:\n","            x_shape = x.size()\n","            if len(x_shape) == 5:\n","                n, ch, d, h, w = x_shape\n","                x = rearrange(x, \"n c d h w -> n d h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n d h w c -> n c d h w\")\n","            elif len(x_shape) == 4:\n","                n, ch, h, w = x_shape\n","                x = rearrange(x, \"n c h w -> n h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n h w c -> n c h w\")\n","        return x\n","\n","    def forward(self, x, normalize=True):\n","        x0 = self.patch_embed(x)\n","        x0 = self.pos_drop(x0)\n","        x0_out = self.proj_out(x0, normalize)\n","        x1 = self.layers1[0](x0.contiguous())\n","        x1_out = self.proj_out(x1, normalize)\n","        x2 = self.layers2[0](x1.contiguous())\n","        x2_out = self.proj_out(x2, normalize)\n","        x3 = self.layers3[0](x2.contiguous())\n","        x3_out = self.proj_out(x3, normalize)\n","        x4 = self.layers4[0](x3.contiguous())\n","        x4_out = self.proj_out(x4, normalize)\n","        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2,4,8,16)).to('cuda')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')\n","# summary(model, (4,155,240,240),1)\n","# n_channels should be in multiples of num_groups (default 8)\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# model = UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')\n","# # n_channels should be in multiples of num_groups (default 8)\n","# print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trainer = Trainer(net=model,\n","#                   dataset=BratsDataset,\n","#                   criterion=BCEDiceLoss(),\n","#                   lr=5e-4,\n","#                   accumulation_steps=4,\n","#                   batch_size=1,\n","#                   val_fold=0,\n","#                   test_fold=1,\n","#                   num_epochs=50,\n","#                   path_to_csv=config.path_to_csv,)\n","DO_RESIZING = True\n","\n","trainer = Trainer(net=model,\n","                  dataset=BratsDataset,\n","                  criterion=BCEDiceLoss(),\n","                  lr=5e-4,\n","                  accumulation_steps=4,\n","                  batch_size=1,\n","                  val_fold=0,\n","                  test_fold=1,\n","                  num_epochs=50,\n","                  path_to_csv=config.path_to_csv,\n","                  do_resizing=DO_RESIZING,\n","                  optimizer=AdamW)\n","\"\"\"UNCOMMENT THE FOLLOWING 2 LINES IF RELOADING MODEL CHECKPOINT\"\"\"\n","# if config.pretrained_model_path is not None:\n","#     trainer.load_predtrain_model(config.pretrained_model_path)\n","\n","# if need - load the logs.\n","# train_logs = pd.read_csv(config.train_logs_path)\n","# trainer.losses[\"train\"] =  train_logs.loc[:, \"train_loss\"].to_list()\n","# trainer.losses[\"val\"] =  train_logs.loc[:, \"val_loss\"].to_list()\n","# trainer.dice_scores[\"train\"] = train_logs.loc[:, \"train_dice\"].to_list()\n","# trainer.dice_scores[\"val\"] = train_logs.loc[:, \"val_dice\"].to_list()\n","# trainer.jaccard_scores[\"train\"] = train_logs.loc[:, \"train_jaccard\"].to_list()\n","# trainer.jaccard_scores[\"val\"] = train_logs.loc[:, \"val_jaccard\"].to_list()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.299558Z","iopub.status.busy":"2023-02-25T14:22:09.299136Z","iopub.status.idle":"2023-02-25T14:22:09.326925Z","shell.execute_reply":"2023-02-25T14:22:09.323871Z","shell.execute_reply.started":"2023-02-25T14:22:09.299518Z"},"trusted":true},"outputs":[],"source":["# dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase='valid', fold=0, batch_size=1)\n","dataloader, _, test_dataloader = get_dataloaders(\n","    dataset=BratsDataset, path_to_csv=config.path_to_csv, val_fold=0, test_fold=1, batch_size=1, do_resizing=DO_RESIZING)\n","len(dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:09.331159Z","iopub.status.busy":"2023-02-25T14:22:09.328642Z","iopub.status.idle":"2023-02-25T14:22:22.153941Z","shell.execute_reply":"2023-02-25T14:22:22.152948Z","shell.execute_reply.started":"2023-02-25T14:22:09.331107Z"},"trusted":true},"outputs":[],"source":["data = next(iter(dataloader))\n","data['Id'], data['image'].shape, data['mask'].shape\n","# size = (batch_size, channels, depth, width, height)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:22:22.155861Z","iopub.status.busy":"2023-02-25T14:22:22.155573Z","iopub.status.idle":"2023-02-25T14:22:24.747368Z","shell.execute_reply":"2023-02-25T14:22:24.746100Z","shell.execute_reply.started":"2023-02-25T14:22:22.155830Z"},"trusted":true},"outputs":[],"source":["img_tensor = data['image'].squeeze()[0].cpu().detach().numpy() \n","mask_tensor = data['mask'].squeeze()[0].squeeze().cpu().detach().numpy()\n","print(\"Num uniq Image values :\", len(np.unique(img_tensor, return_counts=True)[0]))\n","print(\"Min/Max Image values:\", img_tensor.min(), img_tensor.max())\n","print(\"Num uniq Mask values:\", np.unique(mask_tensor, return_counts=True))\n","print(img_tensor.shape)\n","image = np.rot90(montage(img_tensor))\n","mask = np.rot90(montage(mask_tensor)) \n","\n","fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n","ax.imshow(image, cmap ='bone')\n","ax.imshow(np.ma.masked_where(mask == False, mask),\n","           cmap='cool', alpha=0.6)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","## TO-DO: Train your own model; experiment with different hyperparameters\n","\n","### **Optional**: If you don't want to train from scratch, download the 3DUnet model checkpoint trained by the author of the [referenced notebook](https://www.kaggle.com/code/polomarco/brats20-3dunet-3dautoencoder/notebook) and place it in the same path as `config.pretrained_model_path`\n","\n","### Alternatively, train your own model and reload it from the same place."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"Uncomment these to clear GPU VRAM\"\"\"\n","# del model\n","torch.cuda.empty_cache()\n","torch.cuda.synchronize()\n","# assert(0==1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T14:24:20.715410Z","iopub.status.busy":"2023-02-25T14:24:20.714916Z"},"trusted":true},"outputs":[],"source":["# assert(0==1)\n","\"\"\"UNCOMMENT THIS TO START TRAINING\"\"\"\n","trainer.run()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# write logs to train_logs_path\n","with open(os.path.join(config.train_logs_path, 'trainer_properties.txt'), 'w') as f:\n","  for param, value in trainer.__dict__.items():\n","      if not param.startswith(\"__\"):\n","        f.write(f\"{param}:{value}\\n\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Evaluation and Results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.214845Z","iopub.status.idle":"2023-02-25T14:22:25.215649Z"},"trusted":true},"outputs":[],"source":["def compute_scores_per_classes(model,\n","                               dataloader,\n","                               classes):\n","    \"\"\"\n","    Compute Dice and Jaccard coefficients for each class.\n","    Params:\n","        model: neural net for make predictions.\n","        dataloader: dataset object to load data from.\n","        classes: list with classes.\n","        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n","    \"\"\"\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    dice_scores_per_classes = {key: list() for key in classes}\n","    iou_scores_per_classes = {key: list() for key in classes}\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader):\n","            imgs, targets = data['image'], data['mask']\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            logits = model(imgs)\n","            logits = logits.detach().cpu().numpy()\n","            targets = targets.detach().cpu().numpy()\n","            \n","            dice_scores = dice_coef_metric_per_classes(logits, targets)\n","            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n","\n","            for key in dice_scores.keys():\n","                dice_scores_per_classes[key].extend(dice_scores[key])\n","\n","            for key in iou_scores.keys():\n","                iou_scores_per_classes[key].extend(iou_scores[key])\n","\n","    return dice_scores_per_classes, iou_scores_per_classes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.216921Z","iopub.status.idle":"2023-02-25T14:22:25.217729Z"},"trusted":true},"outputs":[],"source":["# test_dataloader = get_dataloaders(dataset=BratsDataset, path_to_csv=config.path_to_csv, val_fold=0, test_fold=1, batch_size=1)\n","len(dataloader)\n","# model.load_state_dict(torch.load(\n","#     'BraTS2020Logs3DUnetBCEDice_Cropped_Fixed/your_last_epoch_model_20230309-172641.pth', map_location='cpu'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.219050Z","iopub.status.idle":"2023-02-25T14:22:25.219967Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.221207Z","iopub.status.idle":"2023-02-25T14:22:25.221974Z"},"trusted":true},"outputs":[],"source":["# %%time\n","dice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n","    model, test_dataloader, ['WT', 'TC', 'ET']\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.223210Z","iopub.status.idle":"2023-02-25T14:22:25.224003Z"},"trusted":true},"outputs":[],"source":["dice_df = pd.DataFrame(dice_scores_per_classes)\n","dice_df.columns = ['WT dice', 'TC dice', 'ET dice']\n","\n","iou_df = pd.DataFrame(iou_scores_per_classes)\n","iou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard']\n","val_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\n","val_metics_df = val_metics_df.loc[:, ['WT dice', 'WT jaccard', \n","                                      'TC dice', 'TC jaccard', \n","                                      'ET dice', 'ET jaccard']]\n","val_metics_df.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_metics_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_metics_df.to_csv(os.path.join(\n","    config.train_logs_path, \"test_results.csv\"))\n","val_metics_df.mean().to_csv(os.path.join(config.train_logs_path, \"test_results_mean.csv\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.225243Z","iopub.status.idle":"2023-02-25T14:22:25.226040Z"},"trusted":true},"outputs":[],"source":["colors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF']\n","palette = sns.color_palette(colors, 6)\n","\n","fig, ax = plt.subplots(figsize=(12, 6));\n","sns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\n","ax.set_xticklabels(val_metics_df.columns, fontsize=14, rotation=15);\n","ax.set_title(\"Dice and Jaccard Coefficients from Test Set\", fontsize=20)\n","\n","for idx, p in enumerate(ax.patches):\n","        percentage = '{:.1f}%'.format(100 * val_metics_df.mean().values[idx])\n","        x = p.get_x() + p.get_width() / 2 - 0.15\n","        y = p.get_y() + p.get_height()\n","        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n","\n","# fig.savefig(os.path.join(config.train_logs_path, \"result.png\"), format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","# fig.savefig(os.path.join(config.train_logs_path, \"result.svg\"), format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{},"source":["## More visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.227262Z","iopub.status.idle":"2023-02-25T14:22:25.228060Z"},"trusted":true},"outputs":[],"source":["def compute_results(model,\n","                    dataloader,\n","                    treshold=0.33):\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    results = {\"Id\": [],\"image\": [], \"GT\": [],\"Prediction\": []}\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader):\n","            id_, imgs, targets = data['Id'], data['image'], data['mask']\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            logits = model(imgs)\n","            probs = torch.sigmoid(logits)\n","            \n","            predictions = (probs >= treshold).float()\n","            predictions =  predictions.cpu()\n","            targets = targets.cpu()\n","            \n","            results[\"Id\"].append(id_)\n","            results[\"image\"].append(imgs.cpu())\n","            results[\"GT\"].append(targets)\n","            results[\"Prediction\"].append(predictions)\n","            \n","            # only 5 pars\n","            if (i > 5):    \n","                return results\n","        return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.229531Z","iopub.status.idle":"2023-02-25T14:22:25.230446Z"},"trusted":true},"outputs":[],"source":["# %%time\n","results = compute_results(\n","    model, test_dataloader, 0.33)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-25T14:22:25.231640Z","iopub.status.idle":"2023-02-25T14:22:25.232572Z"},"trusted":true},"outputs":[],"source":["# for id_, img, gt, prediction in zip(results['Id'][4:],\n","#                     results['image'][4:],\n","#                     results['GT'][4:],\n","#                     results['Prediction'][4:]\n","#                     ):\n","    \n","#     print(id_)\n","#     name = id_[0]\n","\n","#     img = img.squeeze()[0].cpu().detach().numpy()\n","#     gt = gt.squeeze()[0].squeeze().cpu().detach().numpy()\n","#     prediction = prediction.squeeze()[0].squeeze().cpu().detach().numpy()\n","\n","#     print(np.unique(gt))\n","#     print(np.unique(prediction))\n","    \n","    \n","#     for i in range(40,110+1,5):\n","#       fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n","#       slice_w = i\n","#       ax1.imshow(img[slice_w, :, :], cmap=\"gray\")\n","#       ax1.set_title(f'img_{name}_slice_{i}')\n","#       ax2.imshow(gt[slice_w, :, :])\n","#       ax2.set_title(f'gt_{name}_slice_{i}')\n","#       ax3.imshow(prediction[slice_w, :, :])\n","#       ax3.set_title(f'pred_{name}_slice_{i}')\n","#       fig.savefig(os.path.join(config.train_logs_path, f\"prediction_{name}_slice_{i}.png\"),\n","#                   format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","#     break\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for id_, img, gt, prediction in zip(results['Id'][4:],\n","                                    results['image'][4:],\n","                                    results['GT'][4:],\n","                                    results['Prediction'][4:]\n","                                    ):\n","\n","    print(id_)\n","    name = id_[0]\n","\n","    img = img.squeeze()[0].cpu().detach().numpy()\n","    gt0 = gt[0][0].squeeze().cpu().detach().numpy()\n","    gt1 = gt[0][1].squeeze().cpu().detach().numpy()\n","    gt2 = gt[0][2].squeeze().cpu().detach().numpy()\n","    pred0 = prediction[0][0].squeeze().cpu().detach().numpy()\n","    pred1 = prediction[0][1].squeeze().cpu().detach().numpy()\n","    pred2 = prediction[0][2].squeeze().cpu().detach().numpy()\n","\n","    print(gt.shape)\n","    print(prediction.shape)\n","    print(np.unique(gt))\n","    print(np.unique(prediction))\n","\n","    for i in range(40, 110+1, 5):\n","        fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(\n","            1, 7, figsize=(30, 10))\n","        slice_w = i\n","        ax1.imshow(img[slice_w, :, :], cmap=\"gray\")\n","        ax1.set_title(f'img_{name}_slice_{i}')\n","        ax2.imshow(gt0[slice_w, :, :], cmap=\"bone\")\n","        ax2.set_title(f'WT_GT_{name}_slice_{i}')\n","        ax3.imshow(pred0[slice_w, :, :], cmap=\"bone\")\n","        ax3.set_title(f'WT_Pred_{name}_slice_{i}')\n","        ax4.imshow(gt1[slice_w, :, :], cmap=\"hot\")\n","        ax4.set_title(f'TC_GT_{name}_slice_{i}')\n","        ax5.imshow(pred1[slice_w, :, :], cmap=\"hot\")\n","        ax5.set_title(f'TC_Pred_{name}_slice_{i}')\n","        ax6.imshow(gt2[slice_w, :, :], cmap=\"inferno\")\n","        ax6.set_title(f'ET_GT_{name}_slice_{i}')\n","        ax7.imshow(pred2[slice_w, :, :], cmap=\"inferno\")\n","        ax7.set_title(f'ET_Pred_{name}_slice_{i}')\n","        fig.savefig(os.path.join(config.train_logs_path, f\"prediction_{name}_slice_{i}.png\"),\n","                    format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n","    break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"be758f4e65d39904e29fa102fec3d5df4c28e93d12e2ff9b541d16e38b6ab85f"}}},"nbformat":4,"nbformat_minor":4}
